{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx32UcwtcrK5"
      },
      "source": [
        "## Connect to your Google Drive or any filesystem you are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzIyxG0kMvLg",
        "outputId": "2ccabc0d-67fa-477a-8c27-737ba966fd18"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/gdrive\")\n",
        "base_path = (\n",
        "    Path(\"your_path\") if IN_COLAB else Path(\".\")\n",
        ")\n",
        "\n",
        "#Fix if you want to use a different path\n",
        "output_path = base_path / \"output\"\n",
        "data_path = base_path / \"helm\"\n",
        "output_path.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUZjMcaqM4up"
      },
      "source": [
        "##Put here the file system where you are going to work. However you should have there the cloned repositories of:\n",
        "\n",
        "- https://github.com/RUCAIBox/HaluEval.git\n",
        "\n",
        "You should start inside the HaluEval folder if you do not want to change more things. However is enough with having access to the data folder of the HaluEvalRepository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs_MYWyElf9"
      },
      "source": [
        "## These are the libraries needed to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_IRq_2iM8Oh"
      },
      "source": [
        "## Deep Learning Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3JO8-2WM0Oi",
        "outputId": "3c54bbe0-dcbb-4e0e-b3bc-a909df86084c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (4.39.0)\n",
            "Requirement already satisfied: datasets in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.4.1)\n",
            "Requirement already satisfied: rouge_score in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: requests in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: aiohttp in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: multiprocess in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: xxhash in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
            "Requirement already satisfied: pandas in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: responses<0.19 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: absl-py in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
            "Requirement already satisfied: nltk in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: huggingface_hub in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.22.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
            "Requirement already satisfied: filelock in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: requests in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: accelerate in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.21.0)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "Requirement already satisfied: psutil in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (2.2.1)\n",
            "Requirement already satisfied: huggingface-hub in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: pyyaml in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: filelock in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: fsspec in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: networkx in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: triton==2.2.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: jinja2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: requests in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.21.0\n",
            "    Uninstalling accelerate-0.21.0:\n",
            "      Successfully uninstalled accelerate-0.21.0\n",
            "Successfully installed accelerate-0.29.2\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers[torch] in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (4.39.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: filelock in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (0.29.2)\n",
            "Requirement already satisfied: torch in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from transformers[torch]) (2.2.1)\n",
            "Requirement already satisfied: psutil in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: networkx in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: sympy in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: jinja2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.99)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->transformers[torch]) (2.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: sentencepiece in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.2.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: google in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (3.0.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from google) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from beautifulsoup4->google) (2.5)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: protobuf in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (5.26.0)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets evaluate rouge_score\n",
        "%pip install --upgrade huggingface_hub\n",
        "%pip install accelerate -U\n",
        "%pip install transformers[torch]\n",
        "%pip install sentencepiece\n",
        "%pip install google\n",
        "%pip install protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4I9BbUWNGAT"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S_orD5O6M0Sz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulEMNzTdNK6p"
      },
      "source": [
        "#Reading Dataset\n",
        "\n",
        "`The method loadDataset receieves the path where the datasets json files of the HaluEval repository are. You just need to pass your path and the name of the dataset you are going to use.`\n",
        "\n",
        "##Dataset Names:\n",
        "- summarization\n",
        "- dialogue\n",
        "- qa\n",
        "- general\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BBBHKHm1PLgN"
      },
      "outputs": [],
      "source": [
        "## As a recomendation keep these two with the same naming if you do not want to change many things\n",
        "#datasetNames = ['falcon40b','gptj7b','llamabase7b','llamachat13b', 'mpt7b', 'opt7b'] llamachat7b\n",
        "datasetNames = ['llamachat7b', 'falcon40b','mpt7b','llamachat13b', 'llamabase7b', 'opt7b', 'gptj7b']\n",
        "#datasetNames = ['llamabase7b','llamachat13b']\n",
        "#datasetNames = ['llamabase7b']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z559eoe9EsIw"
      },
      "outputs": [],
      "source": [
        "def loadDataset(datasetNames):\n",
        "    sentences_list = []\n",
        "    labels_list = []\n",
        "    prompt_list = []\n",
        "\n",
        "    for datasetName in datasetNames:\n",
        "        path = data_path / f\"./data/{datasetName}\"\n",
        "\n",
        "        with open(path / \"data.json\", \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "            # Iterate through each entry in the JSON\n",
        "            for key, value in data.items():\n",
        "                prompt = value[\"prompt\"]\n",
        "                for sentence_data in value[\"sentences\"]:\n",
        "                    sentence = sentence_data[\"sentence\"]\n",
        "                    label = sentence_data[\"label\"]\n",
        "\n",
        "                    # Append data to lists\n",
        "                    sentences_list.append(sentence)\n",
        "                    labels_list.append(label)\n",
        "                    prompt_list.append(prompt)\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        {\"Prompt\": prompt_list, \"Sentence\": sentences_list, \"Label\": labels_list}\n",
        "    )\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUanNTojFm-j"
      },
      "source": [
        "## For this particular example we are loading the qa_data.json since is the one that takes the less time to process in case you want to test quickly how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fwC9pOCKM0XJ"
      },
      "outputs": [],
      "source": [
        "test_idx = 0\n",
        "train_dataset_names = datasetNames[:test_idx] + datasetNames[test_idx + 1 :]\n",
        "test_dataset = datasetNames[test_idx : test_idx + 1]\n",
        "assert test_dataset[0] not in train_dataset_names\n",
        "train_data = loadDataset(train_dataset_names)\n",
        "test_data = loadDataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_dataset_names=['falcon40b', 'mpt7b', 'llamachat13b', 'llamabase7b', 'opt7b', 'gptj7b']\n",
            "test_dataset=['llamachat7b']\n"
          ]
        }
      ],
      "source": [
        "print(f'{train_dataset_names=}')\n",
        "print(f'{test_dataset=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jx0urZkZRa9U",
        "outputId": "49d6fd3c-cb94-4961-e110-2f01331273ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prompt</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This is a Wikipedia passage about Phillips ' S...</td>\n",
              "      <td>Phillips'Sound Recording Services was a studio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a Wikipedia passage about Phillips ' S...</td>\n",
              "      <td>The studio was used by The Beatles to record t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is a Wikipedia passage about Phillips ' S...</td>\n",
              "      <td>The Beatles were introduced to Phillips by his...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is a Wikipedia passage about Phillips ' S...</td>\n",
              "      <td>The Beatles recorded \"My Bonnie\" and \"The Sain...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is a Wikipedia passage about Best Day Eve...</td>\n",
              "      <td>It is the first episode of the series to be wr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Prompt  \\\n",
              "0  This is a Wikipedia passage about Phillips ' S...   \n",
              "1  This is a Wikipedia passage about Phillips ' S...   \n",
              "2  This is a Wikipedia passage about Phillips ' S...   \n",
              "3  This is a Wikipedia passage about Phillips ' S...   \n",
              "4  This is a Wikipedia passage about Best Day Eve...   \n",
              "\n",
              "                                            Sentence  Label  \n",
              "0  Phillips'Sound Recording Services was a studio...      0  \n",
              "1  The studio was used by The Beatles to record t...      1  \n",
              "2  The Beatles were introduced to Phillips by his...      1  \n",
              "3  The Beatles recorded \"My Bonnie\" and \"The Sain...      1  \n",
              "4  It is the first episode of the series to be wr...      1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPhslb8B2LK_",
        "outputId": "86b3800a-cb9e-4963-d5b7-6322bf03fe00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3239"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6PgoaCNRCz"
      },
      "source": [
        "#Setting Device to use the GPU\n",
        "\n",
        "We use the T4 GPU in Colab since the heaviest computation for us is the inference of the LLM-Evaluator. Therefore, T4 seem as the better fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpd-VUHOM0bn",
        "outputId": "1707657f-9640-4afc-ebd1-b4932a72d024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA L40S\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M-imxNzmM0du"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5GcfQIoNWfm"
      },
      "source": [
        "## Generic LLMModel class to reuse the functionality of extracting the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q_H0RABQM0f7"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import BartForConditionalGeneration, PegasusForConditionalGeneration\n",
        "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LLMModel:\n",
        "    def __init__(self):\n",
        "        # if self.model_name != 'google/gemma-7b-it':\n",
        "        self.model = self.model.to(device)\n",
        "        pass\n",
        "\n",
        "    def getName(self) -> str:\n",
        "        return self.model_name\n",
        "\n",
        "    def getSanitizedName(self) -> str:\n",
        "        return self.model_name.replace(\"/\", \"__\")\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        pass\n",
        "\n",
        "    ##Move in future commits this method to an utils.py\n",
        "    def truncate_string_by_len(self, s, truncate_len):\n",
        "        words = s.split()\n",
        "        truncated_words = words[:-truncate_len] if truncate_len > 0 else words\n",
        "        return \" \".join(truncated_words)\n",
        "\n",
        "    # Method to get the vocabulary probabilities of the LLM for a given token on the generated text from LLM-Generator\n",
        "    def getVocabProbsAtPos(self, pos, token_probs):\n",
        "        sorted_probs, sorted_indices = torch.sort(token_probs[pos, :], descending=True)\n",
        "        return sorted_probs\n",
        "\n",
        "    def getMaxLength(self):\n",
        "        return self.model.config.max_position_embeddings\n",
        "\n",
        "    # By default knowledge is the empty string. If you want to add extra knowledge you can do it like in the cases of the qa_data.json and dialogue_data.json\n",
        "    def extractFeatures(\n",
        "        self,\n",
        "        knowledge=\"\",\n",
        "        conditionted_text=\"\",\n",
        "        generated_text=\"\",\n",
        "        features_to_extract={},\n",
        "    ):\n",
        "        self.model.eval()\n",
        "\n",
        "        \"\"\"Keep in mind that this truncate only works if the long text is the knowledge or the conditioned_text, but not both.\n",
        "        The case of both does not exist on the HaluEval benchmark, but be aware of it if use in other datasets. \"\"\"\n",
        "\n",
        "        # Also in the case of the LED model, there is no need to truncate the text in the context of this dataset.\n",
        "        total_len = len(knowledge) + len(conditionted_text) + len(generated_text)\n",
        "        truncate_len = min(total_len - self.tokenizer.model_max_length, 0)\n",
        "\n",
        "        # Truncate knowledge in case is too large\n",
        "        knowledge = self.truncate_string_by_len(knowledge, truncate_len // 2)\n",
        "        # Truncate text_A in case is too large\n",
        "        conditionted_text = self.truncate_string_by_len(\n",
        "            conditionted_text, truncate_len - (truncate_len // 2)\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            [knowledge + conditionted_text + generated_text],\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.getMaxLength(),\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        for key in inputs:\n",
        "            inputs[key] = inputs[key].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        probs = probs.to(device)\n",
        "\n",
        "        tokens_generated_length = len(self.tokenizer.tokenize(generated_text))\n",
        "        start_index = logits.shape[1] - tokens_generated_length\n",
        "        conditional_probs = probs[0, start_index :]\n",
        "\n",
        "        token_ids_generated = inputs[\"input_ids\"][0, start_index :].tolist()\n",
        "        token_probs_generated = [\n",
        "            conditional_probs[i, tid].item()\n",
        "            for i, tid in enumerate(token_ids_generated)\n",
        "        ]\n",
        "\n",
        "        tokens_generated = self.tokenizer.convert_ids_to_tokens(token_ids_generated)\n",
        "\n",
        "        minimum_token_prob = min(token_probs_generated)\n",
        "        average_token_prob = sum(token_probs_generated) / len(token_probs_generated)\n",
        "\n",
        "        maximum_diff_with_vocab = -1\n",
        "        minimum_vocab_extreme_diff = 100000000000\n",
        "\n",
        "        if features_to_extract[\"MDVTP\"] == True or features_to_extract[\"MMDVP\"] == True:\n",
        "            size = len(token_probs_generated)\n",
        "            for pos in range(size):\n",
        "                vocabProbs = self.getVocabProbsAtPos(pos, conditional_probs)\n",
        "                maximum_diff_with_vocab = max(\n",
        "                    [\n",
        "                        maximum_diff_with_vocab,\n",
        "                        self.getDiffVocab(vocabProbs, token_probs_generated[pos]),\n",
        "                    ]\n",
        "                )\n",
        "                minimum_vocab_extreme_diff = min(\n",
        "                    [\n",
        "                        minimum_vocab_extreme_diff,\n",
        "                        self.getDiffMaximumWithMinimum(vocabProbs),\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "        # allFeatures = [minimum_token_prob, average_token_prob, maximum_diff_with_vocab, minimum_vocab_extreme_diff]\n",
        "\n",
        "        allFeatures = {\n",
        "            \"mtp\": minimum_token_prob,\n",
        "            \"avgtp\": average_token_prob,\n",
        "            \"MDVTP\": maximum_diff_with_vocab,\n",
        "            \"MMDVP\": minimum_vocab_extreme_diff,\n",
        "        }\n",
        "\n",
        "        selectedFeatures = {}\n",
        "        for key, feature in features_to_extract.items():\n",
        "            if feature == True:\n",
        "                selectedFeatures[key] = allFeatures[key]\n",
        "\n",
        "        return selectedFeatures\n",
        "\n",
        "    def getDiffVocab(self, vocabProbs, tprob):\n",
        "        return (vocabProbs[0] - tprob).item()\n",
        "\n",
        "    def getDiffMaximumWithMinimum(self, vocabProbs):\n",
        "        return (vocabProbs[0] - vocabProbs[-1]).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTqGHAYvIyEc"
      },
      "source": [
        "## Definition of the specific Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DTxfgFM3Iwuh"
      },
      "outputs": [],
      "source": [
        "\"\"\"For now there is code repetition, but it helps to understand the details of each model as separate. However, will make\n",
        "everything with better programming practices by using the AutoModel alternatives of HuggingFace.\"\"\"\n",
        "\n",
        "\n",
        "class Gemma(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"google/gemma-7b-it\"\n",
        "        # self.model_name = \"chavinlo/alpaca-native\"\n",
        "        # self.model = AutoModelForCausalLM.from_pretrained(\n",
        "        #     self.model_name,\n",
        "            # low_cpu_mem_usage=True,\n",
        "            # torch_dtype=torch.float16,\n",
        "            # load_in_4bit=True,\n",
        "        # )\n",
        "\n",
        "        # quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name\n",
        "        )  # , #torch_dtype=torch.float16,# quantization_config=quantization_config)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class LLama(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "        # self.model_name = \"chavinlo/alpaca-native\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            # low_cpu_mem_usage=True,\n",
        "            # torch_dtype=torch.float16,\n",
        "            # load_in_4bit=True,\n",
        "        )\n",
        "        # self.model = LlamaForCausalLM.from_pretrained(\n",
        "        #     self.model_name\n",
        "        #     # ,torch_dtype=torch.float16\n",
        "        # )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=1024, return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class Opt(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/opt-6.7b\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        " \n",
        " \n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        " \n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        " \n",
        "        return summary\n",
        "\n",
        "\n",
        "class Gptj(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"EleutherAI/gpt-j-6B\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        " \n",
        " \n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        " \n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        " \n",
        "        return summary\n",
        "\n",
        "\n",
        "class BartCNN(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class BartCNNLong(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"ccdv/lsg-bart-base-16384-arxiv\"\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer([inpt], return_tensors=\"pt\", truncation=True)\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class GPT2Generator(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gpt2-large\"\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer.encode(\n",
        "            inpt, return_tensors=\"pt\", max_length=self.getMaxLength(), truncation=True\n",
        "        )\n",
        "        output_ids = self.model.generate(\n",
        "            inputs, max_length=1024, num_return_sequences=1\n",
        "        )\n",
        "        output = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LED(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"allenai/led-large-16384-arxiv\"\n",
        "        self.model = LEDForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.tokenizer = LEDTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtKuMaZNQ8MO"
      },
      "source": [
        "# The Dictionary `features_to_extract` defines which features will be use in this experiment.\n",
        "\n",
        "## Features Meaning:\n",
        "\n",
        "- `mtp` : Take the minimum of the probabilities that the LLM_E gives to the tokens on the generated-text.\n",
        "- `avgtp` : Take the average of the probabilities that the LLM_E\n",
        "gives to the tokens on the generated-text.\n",
        "- `MDVTP` : Take the maximum from all the differences\n",
        "between the token with the highest probability\n",
        "according to LLM_E at position i and the\n",
        "assigned probability from LLM_E to the token at position i in the generated_text.\n",
        "- `MMDVP` : Take the maximum from all the differences between the token with the highest probability according to $LLM_E$ at position $i$ ($v^*$) and the token with the lowest probability according to $LLM_E$ at position $i$ ($v^-$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5OjJxW1PRHrE"
      },
      "outputs": [],
      "source": [
        "feature_to_extract = 'all'\n",
        "\n",
        "available_features_to_extract = [\"mtp\", \"avgtp\", \"MDVTP\", \"MMDVP\"]\n",
        "if feature_to_extract == 'all':\n",
        "    features_to_extract = {\n",
        "        feature: True for feature in available_features_to_extract\n",
        "    }\n",
        "else:\n",
        "    features_to_extract = {\n",
        "        feature: True if feature == feature_to_extract else False\n",
        "        for feature in available_features_to_extract\n",
        "    }\n",
        "\n",
        "features_to_extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GdklJnQIB8"
      },
      "source": [
        "## This cell is to instantiate the model you intend to use for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekiYqeNCUQ_q",
        "outputId": "2429fe9e-d57f-4696-db23-7259dfa87479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: huggingface-hub>=0.17.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (0.22.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (4.10.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (2024.2.0)\n",
            "Requirement already satisfied: requests in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (6.0.1)\n",
            "Requirement already satisfied: filelock in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from huggingface-hub>=0.17.1) (3.13.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.1) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.1) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/yeroj/.venv/hallu/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.1) (2024.2.2)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/data/yeroj/.venv/hallu/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2\n",
        "%pip install \"huggingface-hub>=0.17.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hvyg_GUQRT",
        "outputId": "ef3e92f0-cd8f-4f64-eed3-3bfbe2c8f5a1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99a69e51dc864f5b9c30297d1c11b358",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TZHYra4sM0ig"
      },
      "outputs": [],
      "source": [
        "# model = BartCNN()\n",
        "# model = BrioBart()\n",
        "# model = LED()\n",
        "# model = GPT2Generator()\n",
        "model = LLama()\n",
        "# model = Gemma()\n",
        "# model = Opt()\n",
        "# model = Gptj()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgpfLjKQiwe"
      },
      "source": [
        "##Cleaning Cache on GPU to save memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4t5DYYzaM0nE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUXW1LNxQnal"
      },
      "source": [
        "#This cell creates the dataset separation of `10%` for training and `90%` for testing depending on what task you are addressing. The following explanation is what happens if summarization is the task used. But the same explanation applies to all tasks and also you cand pass as parameter how many data points you want to include in training.\n",
        "\n",
        "## Example: The data is separated on 2000 (1000 of document with right summary and 1000 with the same document but with the hallucinated summary). The rest which is 18000 is used to for testing.\n",
        "\n",
        "### As expected from previous cells the task string expected are:\n",
        "- `summarization`\n",
        "- `qa`\n",
        "- `dialogue`\n",
        "- `general`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "includeConditioned = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JCPqy3UTLrJa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def adaptDataset(\n",
        "    train_data: pd.DataFrame, test_data: pd.DataFrame, includeConditioned: bool\n",
        "):\n",
        "\n",
        "    dataset_train = []\n",
        "    dataset_test = []\n",
        "    for _, row in train_data.iterrows():\n",
        "        prompt, text, hallu = row[\"Prompt\"], row[\"Sentence\"], row[\"Label\"]\n",
        "        dataset_train.append((prompt, text, hallu))\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        prompt, text, hallu = row[\"Prompt\"], row[\"Sentence\"], row[\"Label\"]\n",
        "        dataset_test.append((prompt, text, hallu))\n",
        "\n",
        "    random.shuffle(dataset_train)\n",
        "    random.shuffle(dataset_test)\n",
        "\n",
        "    X_train = [(p if includeConditioned else \"\", t) for p, t, _ in dataset_train]\n",
        "    Y_train = [y for _, _, y, in dataset_train]\n",
        "\n",
        "    X_test = [(p if includeConditioned else \"\", t) for p, t, _ in dataset_test]\n",
        "    Y_test = [y for _, _, y, in dataset_test]\n",
        "\n",
        "    return X_train, Y_train, [], [], X_test, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dBUtDsPsPFy3"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train, X_val, Y_val, X_test, Y_test = adaptDataset(train_data, test_data, includeConditioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mHLWT2nrcli",
        "outputId": "3b8492a2-95d5-42db-829f-45ef5aa64269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3239 3239\n",
            "0 0\n",
            "497 497\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))\n",
        "print(len(X_test), len(Y_test)) #verify the sizes look right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2BNY5b1M0ri",
        "outputId": "94bbaeee-9be4-4167-d1b2-93c0b5f0aabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3239 3239\n",
            "0 0\n",
            "497 497\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))\n",
        "print(len(X_test), len(Y_test)) #verify the sizes look right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDHkOclDQMDW",
        "outputId": "53192fee-05ca-4d31-fb0e-69cf3659a6ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('This is a Wikipedia passage about Moaning Lisa. \" Moaning Lisa \" is the sixth episode of The Simpsons \\' first season .',\n",
              " 'It originally aired on the Fox network in the United States on November 25, 1989 .\\nlabel0\\ntext: \"1989\"\\n\\nThe episode was written by Conan O\\'Brien and John Swartzwelder and directed by Rich Moore .')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo_9Sd3n9BYA",
        "outputId": "96014aed-6618-4d8f-efc4-154311d88c7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNNRZuisSUrf"
      },
      "source": [
        "## Extracting the features for the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtdBbmC8QgoK",
        "outputId": "214d8aef-9bbe-4a50-9ae4-4926ae7d006c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   0%|          | 0/3239 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing:   1%|         | 43/3239 [03:30<4:21:03,  4.90s/it]  \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# i = 0\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conditioned_text, generated_text \u001b[38;5;129;01min\u001b[39;00m tqdm(X_train, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# print(\"Extracting: \", i)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     X_train_features_maps\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 21\u001b[0m         \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditioned_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_extract\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clean cache in every step for memory saving.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# i += 1\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[26], line 11\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(knowledge, conditioned_text, generated_text, features_to_extract)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\n\u001b[1;32m      6\u001b[0m     knowledge: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      7\u001b[0m     conditioned_text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     generated_text: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      9\u001b[0m     features_to_extract: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m     10\u001b[0m ):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractFeatures\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknowledge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditioned_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_extract\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 78\u001b[0m, in \u001b[0;36mLLMModel.extractFeatures\u001b[0;34m(self, knowledge, conditionted_text, generated_text, features_to_extract)\u001b[0m\n\u001b[1;32m     75\u001b[0m     inputs[key] \u001b[38;5;241m=\u001b[39m inputs[key]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 78\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     81\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:1130\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1130\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:956\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    945\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    946\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    947\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    953\u001b[0m         output_attentions,\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 956\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:584\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 584\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    594\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:257\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    254\u001b[0m q_pass \u001b[38;5;241m=\u001b[39m query[:, :, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_dim :]\n\u001b[1;32m    256\u001b[0m k_rot \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(k_rot, sin, cos)\n\u001b[0;32m--> 257\u001b[0m q_rot \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_rot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([k_rot, k_pass], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    260\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([q_rot, q_pass], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/data/yeroj/.venv/hallu/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:99\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(tensor, sin, cos)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(tensor: torch\u001b[38;5;241m.\u001b[39mTensor, sin: torch\u001b[38;5;241m.\u001b[39mTensor, cos: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     98\u001b[0m     sin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(sin[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m     cos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tensor \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_every_two(tensor) \u001b[38;5;241m*\u001b[39m sin)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def extract_features(\n",
        "    knowledge: str,\n",
        "    conditioned_text: str,\n",
        "    generated_text: str,\n",
        "    features_to_extract: dict[str, bool],\n",
        "):\n",
        "    return model.extractFeatures(\n",
        "        knowledge, conditioned_text, generated_text, features_to_extract\n",
        "    )\n",
        "\n",
        "X_train_features_maps = []\n",
        "# i = 0\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_train, desc=\"Processing\"):\n",
        "    # print(\"Extracting: \", i)\n",
        "    X_train_features_maps.append(\n",
        "        extract_features(\n",
        "            \"\", conditioned_text, generated_text, features_to_extract\n",
        "        )\n",
        "    )\n",
        "    torch.cuda.empty_cache()  # Clean cache in every step for memory saving.\n",
        "    # i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYazbv-mQgqe",
        "outputId": "6413557b-8079-4ee4-b3e8-501e688ac7ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3170"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train_features_maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCLqbQsU9rjT",
        "outputId": "62391d9b-1ff6-43de-f22e-90e9fbf8697a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mtp': 2.7901327914747753e-09,\n",
              " 'avgtp': 5.7490163297047623e-05,\n",
              " 'MDVTP': 0.9977340698242188,\n",
              " 'MMDVP': 0.07783844321966171}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_features_maps[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FRHI34IE9uMQ"
      },
      "outputs": [],
      "source": [
        "X_train_features = [list(dic.values()) for dic in X_train_features_maps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eWIgjAU-UW9",
        "outputId": "244db606-1025-425a-e02f-5ac6632c4fe5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3170"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFXzlfwd-UYy",
        "outputId": "dd65859a-96e9-4b6a-8bdd-0d340eac1bce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2.7901327914747753e-09,\n",
              " 5.7490163297047623e-05,\n",
              " 0.9977340698242188,\n",
              " 0.07783844321966171]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgd5gk9SjjK"
      },
      "source": [
        "## Training Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UdbK2IsvQgsp",
        "outputId": "bc48fbde-6eb7-49b7-c74b-3c1e6540fc11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUNNING THE L-BFGS-B CODE\n",
            "\n",
            "           * * *\n",
            "\n",
            "Machine precision = 2.220D-16\n",
            " N =            5     M =           10\n",
            "\n",
            "At X0         0 variables are exactly at the bounds\n",
            "\n",
            "At iterate    0    f=  6.93147D-01    |proj g|=  7.69127D-03\n",
            "\n",
            "           * * *\n",
            "\n",
            "Tit   = total number of iterations\n",
            "Tnf   = total number of function evaluations\n",
            "Tnint = total number of segments explored during Cauchy searches\n",
            "Skip  = number of BFGS updates skipped\n",
            "Nact  = number of active bounds at final generalized Cauchy point\n",
            "Projg = norm of the final projected gradient\n",
            "F     = final function value\n",
            "\n",
            "           * * *\n",
            "\n",
            "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
            "    5      9     12      1     0     0   3.212D-05   6.686D-01\n",
            "  F =  0.66863515233775161     \n",
            "\n",
            "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " This problem is unconstrained.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(verbose=1)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(verbose=1)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(verbose=1)\n",
        "clf.fit(X_train_features, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O69UE4MUS5_v"
      },
      "source": [
        "## Evaluate accuracy of Logistic Regression on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA6ovXb2S5kQ",
        "outputId": "a1172515-4b1e-4d36-f582-edf438caa21f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 58.93%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Y_Pred = clf.predict(X_train_features)\n",
        "\n",
        "accuracy = accuracy_score(Y_train, Y_Pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "asLAVNjYnMzX",
        "outputId": "976a42f0-f220-4bf7-b6ee-6865271abf8b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MDVTP</th>\n",
              "      <td>0.030913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mtp</th>\n",
              "      <td>-0.000034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgtp</th>\n",
              "      <td>-0.178024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MMDVP</th>\n",
              "      <td>-6.314978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef\n",
              "MDVTP  0.030913\n",
              "mtp   -0.000034\n",
              "avgtp -0.178024\n",
              "MMDVP -6.314978"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_odds = clf.coef_[0]\n",
        "odds = np.exp(clf.coef_[0])\n",
        "lr_features_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), log_odds)}\n",
        "lr_features_no_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), odds)}\n",
        "\n",
        "print(\"log\", lr_features_log)\n",
        "print(\"no_log\", lr_features_no_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCtQNHq9zVDv"
      },
      "source": [
        "## Extracting the Features of the Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-kRb2gzUE1",
        "outputId": "598aa3d8-b05b-456f-e749-afa3d5655973"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "X_val_features_map = []\n",
        "# i = 0\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_val, desc=\"Processing\"):\n",
        "    # print(\"Extracting: \", i)\n",
        "    X_val_features_map.append(\n",
        "        extract_features(\"\", conditioned_text, generated_text, features_to_extract)\n",
        "    )\n",
        "    torch.cuda.empty_cache()\n",
        "    # i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dI45BIF9zUHO"
      },
      "outputs": [],
      "source": [
        "X_val_features = [list(dic.values()) for dic in X_val_features_map]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "W4t1F1y9zUJf",
        "outputId": "c85b0afd-8704-48ba-d417-03cbef42d33a"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Y_Pred = clf.predict(X_val_features)\n",
        "\n",
        "# accuracy = accuracy_score(Y_val, Y_Pred)\n",
        "# print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITx_0w7BTCce"
      },
      "source": [
        "## Extracting the Features of the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg60b4WcS5Nw",
        "outputId": "68a385eb-cbf3-4d90-c2ab-6912397e5e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|| 566/566 [00:44<00:00, 12.62it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_test_features_map = []\n",
        "# i = 0\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_test, desc=\"Processing\"):\n",
        "    # print(\"Extracting: \", i)\n",
        "    X_test_features_map.append(\n",
        "        extract_features(\n",
        "            \"\", conditioned_text, generated_text, features_to_extract\n",
        "        )\n",
        "    )\n",
        "    torch.cuda.empty_cache()\n",
        "    # i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kOmEOFRY-fci"
      },
      "outputs": [],
      "source": [
        "X_test_features = [list(dic.values()) for dic in X_test_features_map]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaW4-c3XSzfP"
      },
      "source": [
        "## Evaluate accuracy of the LogisticRegression on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6_VRaKTQgu8",
        "outputId": "54bf8d4e-8a6b-4cee-cebc-d0664e6f2d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 66.25%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Y_Pred = clf.predict(X_test_features)\n",
        "\n",
        "lr_accuracy = accuracy_score(Y_test, Y_Pred)\n",
        "print(f\"Accuracy: {lr_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "8pMh03rcrs6m",
        "outputId": "cf86e151-5518-42a0-be4c-a33c2b23af9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MDVTP</th>\n",
              "      <td>0.030913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mtp</th>\n",
              "      <td>-0.000034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgtp</th>\n",
              "      <td>-0.178024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MMDVP</th>\n",
              "      <td>-6.314978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef\n",
              "MDVTP  0.030913\n",
              "mtp   -0.000034\n",
              "avgtp -0.178024\n",
              "MMDVP -6.314978"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_odds = clf.coef_[0]\n",
        "pd.DataFrame(log_odds,\n",
        "             X_train_features_maps[0].keys(),\n",
        "             columns=['coef'])\\\n",
        "            .sort_values(by='coef', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "E4b53IT53_B7",
        "outputId": "4bec1773-3965-4284-d5a4-569912c378eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MDVTP</th>\n",
              "      <td>1.031396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mtp</th>\n",
              "      <td>0.999967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>avgtp</th>\n",
              "      <td>0.836922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MMDVP</th>\n",
              "      <td>0.001809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef\n",
              "MDVTP  1.031396\n",
              "mtp    0.999967\n",
              "avgtp  0.836922\n",
              "MMDVP  0.001809"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "odds = np.exp(clf.coef_[0])\n",
        "pd.DataFrame(odds,\n",
        "             X_train_features_maps[0].keys(),\n",
        "             columns=['coef'])\\\n",
        "            .sort_values(by='coef', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5FFCFqGLTMez"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleDenseNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1, dropout_prob=0.3):\n",
        "        super(SimpleDenseNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DICECZu0Qgw6"
      },
      "outputs": [],
      "source": [
        "denseModel = SimpleDenseNet(input_dim=len(list(features_to_extract.keys())), hidden_dim=512).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FeJQkPQQjFM"
      },
      "source": [
        "#Code declaring and computing all the metrics to measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Zb_0po_9QiKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        "    auc\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(model, input_tensor, true_labels):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predicted_probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        predicted = (outputs > 0.5).float().cpu().numpy()\n",
        "\n",
        "        true_labels = true_labels.cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(true_labels, predicted)\n",
        "        precision = precision_score(true_labels, predicted)\n",
        "        recall = recall_score(true_labels, predicted)\n",
        "        f1 = f1_score(true_labels, predicted)\n",
        "\n",
        "        precision_negative = precision_score(true_labels, predicted, pos_label=0)\n",
        "        recall_negative = recall_score(true_labels, predicted, pos_label=0)\n",
        "        f1_negative = f1_score(true_labels, predicted, pos_label=0)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(true_labels, predicted).ravel()\n",
        "        roc_auc = roc_auc_score(true_labels, predicted_probs)\n",
        "\n",
        "        P, R, _ = precision_recall_curve(true_labels, predicted, pos_label=1)\n",
        "        pr_auc = auc(R, P)\n",
        "\n",
        "        roc_auc_negative = roc_auc_score(\n",
        "            true_labels, 1 - predicted_probs\n",
        "        )  # If predicted_probs is the probability of the positive class\n",
        "        P_neg, R_neg, _ = precision_recall_curve(true_labels, predicted, pos_label=0)\n",
        "        pr_auc_negative = auc(R_neg, P_neg)\n",
        "\n",
        "        return {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1\": f1,\n",
        "            \"TP\": tp,\n",
        "            \"TN\": tn,\n",
        "            \"FP\": fp,\n",
        "            \"FN\": fn,\n",
        "            \"ROC AUC\": roc_auc,\n",
        "            \"PR AUC\": pr_auc,\n",
        "            \"Precision-Negative\": precision_negative,\n",
        "            \"Recall-Negative\": recall_negative,\n",
        "            \"F1-Negative\": f1_negative,\n",
        "            \"ROC AUC-Negative\": roc_auc_negative,\n",
        "            \"PR AUC-Negative\": pr_auc_negative,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Fi6-lJUP11"
      },
      "source": [
        "## Code for training the Dense Model and getting the result of all metrics corresponding to the Testing Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNtGdq4b-8wo",
        "outputId": "2ec2c5fc-14cc-482a-fd24-dbab9b9557f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3170, 4]) torch.Size([3170, 1])\n",
            "Epoch [10/10000], Loss: 0.6880, Training Accuracy: 0.5558\n",
            "Epoch [20/10000], Loss: 0.6770, Training Accuracy: 0.5921\n",
            "Epoch [30/10000], Loss: 0.6661, Training Accuracy: 0.5927\n",
            "Epoch [40/10000], Loss: 0.6599, Training Accuracy: 0.5959\n",
            "Epoch [50/10000], Loss: 0.6599, Training Accuracy: 0.5924\n",
            "Epoch [60/10000], Loss: 0.6591, Training Accuracy: 0.5924\n",
            "Epoch [70/10000], Loss: 0.6588, Training Accuracy: 0.5927\n",
            "Epoch [80/10000], Loss: 0.6586, Training Accuracy: 0.5937\n",
            "Epoch [90/10000], Loss: 0.6584, Training Accuracy: 0.5931\n",
            "Epoch [100/10000], Loss: 0.6582, Training Accuracy: 0.5937\n",
            "Epoch [110/10000], Loss: 0.6580, Training Accuracy: 0.5950\n",
            "Epoch [120/10000], Loss: 0.6577, Training Accuracy: 0.5950\n",
            "Epoch [130/10000], Loss: 0.6575, Training Accuracy: 0.5953\n",
            "Epoch [140/10000], Loss: 0.6572, Training Accuracy: 0.5959\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [150/10000], Loss: 0.6569, Training Accuracy: 0.5968\n",
            "Epoch [160/10000], Loss: 0.6566, Training Accuracy: 0.5943\n",
            "Epoch [170/10000], Loss: 0.6563, Training Accuracy: 0.5959\n",
            "Epoch [180/10000], Loss: 0.6562, Training Accuracy: 0.5991\n",
            "Epoch [190/10000], Loss: 0.6556, Training Accuracy: 0.6013\n",
            "Epoch [200/10000], Loss: 0.6553, Training Accuracy: 0.5975\n",
            "Epoch [210/10000], Loss: 0.6550, Training Accuracy: 0.6016\n",
            "Epoch [220/10000], Loss: 0.6548, Training Accuracy: 0.5968\n",
            "Epoch [230/10000], Loss: 0.6542, Training Accuracy: 0.5978\n",
            "Epoch [240/10000], Loss: 0.6535, Training Accuracy: 0.6003\n",
            "Epoch [250/10000], Loss: 0.6532, Training Accuracy: 0.6000\n",
            "Epoch [260/10000], Loss: 0.6541, Training Accuracy: 0.5968\n",
            "Epoch [270/10000], Loss: 0.6525, Training Accuracy: 0.5981\n",
            "Epoch [280/10000], Loss: 0.6518, Training Accuracy: 0.6038\n",
            "Epoch [290/10000], Loss: 0.6515, Training Accuracy: 0.6032\n",
            "Epoch [300/10000], Loss: 0.6510, Training Accuracy: 0.6044\n",
            "Epoch [310/10000], Loss: 0.6506, Training Accuracy: 0.6063\n",
            "Epoch [320/10000], Loss: 0.6517, Training Accuracy: 0.5909\n",
            "Epoch [330/10000], Loss: 0.6510, Training Accuracy: 0.6044\n",
            "Epoch [340/10000], Loss: 0.6503, Training Accuracy: 0.6095\n",
            "Epoch [350/10000], Loss: 0.6497, Training Accuracy: 0.6091\n",
            "Epoch [360/10000], Loss: 0.6492, Training Accuracy: 0.6085\n",
            "Epoch [370/10000], Loss: 0.6490, Training Accuracy: 0.6107\n",
            "Epoch [380/10000], Loss: 0.6488, Training Accuracy: 0.6114\n",
            "Epoch [390/10000], Loss: 0.6486, Training Accuracy: 0.6098\n",
            "Epoch [400/10000], Loss: 0.6484, Training Accuracy: 0.6079\n",
            "Epoch [410/10000], Loss: 0.6482, Training Accuracy: 0.6073\n",
            "Epoch [420/10000], Loss: 0.6481, Training Accuracy: 0.6079\n",
            "Epoch [430/10000], Loss: 0.6479, Training Accuracy: 0.6076\n",
            "Epoch [440/10000], Loss: 0.6478, Training Accuracy: 0.6076\n",
            "Epoch [450/10000], Loss: 0.6479, Training Accuracy: 0.6082\n",
            "Epoch [460/10000], Loss: 0.6494, Training Accuracy: 0.6044\n",
            "Epoch [470/10000], Loss: 0.6478, Training Accuracy: 0.6050\n",
            "Epoch [480/10000], Loss: 0.6479, Training Accuracy: 0.6066\n",
            "Epoch [490/10000], Loss: 0.6476, Training Accuracy: 0.6082\n",
            "Epoch [500/10000], Loss: 0.6475, Training Accuracy: 0.6066\n",
            "Epoch [510/10000], Loss: 0.6474, Training Accuracy: 0.6063\n",
            "Epoch [520/10000], Loss: 0.6473, Training Accuracy: 0.6063\n",
            "Epoch [530/10000], Loss: 0.6473, Training Accuracy: 0.6066\n",
            "Epoch [540/10000], Loss: 0.6473, Training Accuracy: 0.6066\n",
            "Epoch [550/10000], Loss: 0.6472, Training Accuracy: 0.6060\n",
            "Epoch [560/10000], Loss: 0.6472, Training Accuracy: 0.6063\n",
            "Epoch [570/10000], Loss: 0.6471, Training Accuracy: 0.6050\n",
            "Epoch [580/10000], Loss: 0.6472, Training Accuracy: 0.6088\n",
            "Epoch [590/10000], Loss: 0.6474, Training Accuracy: 0.6044\n",
            "Epoch [600/10000], Loss: 0.6480, Training Accuracy: 0.6110\n",
            "Epoch [610/10000], Loss: 0.6470, Training Accuracy: 0.6079\n",
            "Epoch [620/10000], Loss: 0.6470, Training Accuracy: 0.6066\n",
            "Epoch [630/10000], Loss: 0.6469, Training Accuracy: 0.6088\n",
            "Epoch [640/10000], Loss: 0.6469, Training Accuracy: 0.6066\n",
            "Epoch [650/10000], Loss: 0.6469, Training Accuracy: 0.6063\n",
            "Epoch [660/10000], Loss: 0.6468, Training Accuracy: 0.6079\n",
            "Epoch [670/10000], Loss: 0.6468, Training Accuracy: 0.6091\n",
            "Epoch [680/10000], Loss: 0.6468, Training Accuracy: 0.6082\n",
            "Epoch [690/10000], Loss: 0.6467, Training Accuracy: 0.6088\n",
            "Epoch [700/10000], Loss: 0.6467, Training Accuracy: 0.6088\n",
            "Epoch [710/10000], Loss: 0.6467, Training Accuracy: 0.6095\n",
            "Epoch [720/10000], Loss: 0.6467, Training Accuracy: 0.6088\n",
            "Epoch [730/10000], Loss: 0.6470, Training Accuracy: 0.6123\n",
            "Epoch [740/10000], Loss: 0.6469, Training Accuracy: 0.6050\n",
            "Epoch [750/10000], Loss: 0.6467, Training Accuracy: 0.6073\n",
            "Epoch [760/10000], Loss: 0.6467, Training Accuracy: 0.6063\n",
            "Epoch [770/10000], Loss: 0.6466, Training Accuracy: 0.6079\n",
            "Epoch [780/10000], Loss: 0.6466, Training Accuracy: 0.6057\n",
            "Epoch [790/10000], Loss: 0.6465, Training Accuracy: 0.6088\n",
            "Epoch [800/10000], Loss: 0.6465, Training Accuracy: 0.6085\n",
            "Epoch [810/10000], Loss: 0.6465, Training Accuracy: 0.6088\n",
            "Epoch [820/10000], Loss: 0.6464, Training Accuracy: 0.6091\n",
            "Epoch [830/10000], Loss: 0.6464, Training Accuracy: 0.6073\n",
            "Epoch [840/10000], Loss: 0.6464, Training Accuracy: 0.6082\n",
            "Epoch [850/10000], Loss: 0.6464, Training Accuracy: 0.6085\n",
            "Epoch [860/10000], Loss: 0.6468, Training Accuracy: 0.6076\n",
            "Epoch [870/10000], Loss: 0.6463, Training Accuracy: 0.6104\n",
            "Epoch [880/10000], Loss: 0.6467, Training Accuracy: 0.6060\n",
            "Epoch [890/10000], Loss: 0.6464, Training Accuracy: 0.6060\n",
            "Epoch [900/10000], Loss: 0.6462, Training Accuracy: 0.6063\n",
            "Epoch [910/10000], Loss: 0.6462, Training Accuracy: 0.6088\n",
            "Epoch [920/10000], Loss: 0.6461, Training Accuracy: 0.6076\n",
            "Epoch [930/10000], Loss: 0.6461, Training Accuracy: 0.6069\n",
            "Epoch [940/10000], Loss: 0.6461, Training Accuracy: 0.6076\n",
            "Epoch [950/10000], Loss: 0.6461, Training Accuracy: 0.6076\n",
            "Epoch [960/10000], Loss: 0.6471, Training Accuracy: 0.6066\n",
            "Epoch [970/10000], Loss: 0.6471, Training Accuracy: 0.6063\n",
            "Epoch [980/10000], Loss: 0.6461, Training Accuracy: 0.6126\n",
            "Epoch [990/10000], Loss: 0.6460, Training Accuracy: 0.6120\n",
            "Epoch [1000/10000], Loss: 0.6460, Training Accuracy: 0.6101\n",
            "Epoch [1010/10000], Loss: 0.6459, Training Accuracy: 0.6085\n",
            "Epoch [1020/10000], Loss: 0.6459, Training Accuracy: 0.6088\n",
            "Epoch [1030/10000], Loss: 0.6459, Training Accuracy: 0.6085\n",
            "Epoch [1040/10000], Loss: 0.6459, Training Accuracy: 0.6076\n",
            "Epoch [1050/10000], Loss: 0.6458, Training Accuracy: 0.6085\n",
            "Epoch [1060/10000], Loss: 0.6458, Training Accuracy: 0.6076\n",
            "Epoch [1070/10000], Loss: 0.6458, Training Accuracy: 0.6076\n",
            "Epoch [1080/10000], Loss: 0.6458, Training Accuracy: 0.6082\n",
            "Epoch [1090/10000], Loss: 0.6460, Training Accuracy: 0.6098\n",
            "Epoch [1100/10000], Loss: 0.6460, Training Accuracy: 0.6098\n",
            "Epoch [1110/10000], Loss: 0.6464, Training Accuracy: 0.6069\n",
            "Epoch [1120/10000], Loss: 0.6459, Training Accuracy: 0.6066\n",
            "Epoch [1130/10000], Loss: 0.6457, Training Accuracy: 0.6088\n",
            "Epoch [1140/10000], Loss: 0.6456, Training Accuracy: 0.6079\n",
            "Epoch [1150/10000], Loss: 0.6456, Training Accuracy: 0.6079\n",
            "Epoch [1160/10000], Loss: 0.6455, Training Accuracy: 0.6079\n",
            "Epoch [1170/10000], Loss: 0.6455, Training Accuracy: 0.6076\n",
            "Epoch [1180/10000], Loss: 0.6455, Training Accuracy: 0.6069\n",
            "Epoch [1190/10000], Loss: 0.6455, Training Accuracy: 0.6079\n",
            "Epoch [1200/10000], Loss: 0.6454, Training Accuracy: 0.6091\n",
            "Epoch [1210/10000], Loss: 0.6462, Training Accuracy: 0.6098\n",
            "Epoch [1220/10000], Loss: 0.6455, Training Accuracy: 0.6088\n",
            "Epoch [1230/10000], Loss: 0.6458, Training Accuracy: 0.6120\n",
            "Epoch [1240/10000], Loss: 0.6455, Training Accuracy: 0.6126\n",
            "Epoch [1250/10000], Loss: 0.6453, Training Accuracy: 0.6088\n",
            "Epoch [1260/10000], Loss: 0.6453, Training Accuracy: 0.6082\n",
            "Epoch [1270/10000], Loss: 0.6453, Training Accuracy: 0.6076\n",
            "Epoch [1280/10000], Loss: 0.6452, Training Accuracy: 0.6079\n",
            "Epoch [1290/10000], Loss: 0.6452, Training Accuracy: 0.6082\n",
            "Epoch [1300/10000], Loss: 0.6453, Training Accuracy: 0.6063\n",
            "Epoch [1310/10000], Loss: 0.6460, Training Accuracy: 0.6085\n",
            "Epoch [1320/10000], Loss: 0.6452, Training Accuracy: 0.6104\n",
            "Epoch [1330/10000], Loss: 0.6451, Training Accuracy: 0.6085\n",
            "Epoch [1340/10000], Loss: 0.6451, Training Accuracy: 0.6079\n",
            "Epoch [1350/10000], Loss: 0.6451, Training Accuracy: 0.6085\n",
            "Epoch [1360/10000], Loss: 0.6459, Training Accuracy: 0.6073\n",
            "Epoch [1370/10000], Loss: 0.6451, Training Accuracy: 0.6136\n",
            "Epoch [1380/10000], Loss: 0.6452, Training Accuracy: 0.6079\n",
            "Epoch [1390/10000], Loss: 0.6455, Training Accuracy: 0.6142\n",
            "Epoch [1400/10000], Loss: 0.6451, Training Accuracy: 0.6082\n",
            "Epoch [1410/10000], Loss: 0.6450, Training Accuracy: 0.6104\n",
            "Epoch [1420/10000], Loss: 0.6449, Training Accuracy: 0.6088\n",
            "Epoch [1430/10000], Loss: 0.6448, Training Accuracy: 0.6076\n",
            "Epoch [1440/10000], Loss: 0.6448, Training Accuracy: 0.6104\n",
            "Epoch [1450/10000], Loss: 0.6454, Training Accuracy: 0.6139\n",
            "Epoch [1460/10000], Loss: 0.6448, Training Accuracy: 0.6069\n",
            "Epoch [1470/10000], Loss: 0.6448, Training Accuracy: 0.6079\n",
            "Epoch [1480/10000], Loss: 0.6448, Training Accuracy: 0.6114\n",
            "Epoch [1490/10000], Loss: 0.6447, Training Accuracy: 0.6085\n",
            "Epoch [1500/10000], Loss: 0.6449, Training Accuracy: 0.6091\n",
            "Epoch [1510/10000], Loss: 0.6450, Training Accuracy: 0.6088\n",
            "Epoch [1520/10000], Loss: 0.6447, Training Accuracy: 0.6073\n",
            "Epoch [1530/10000], Loss: 0.6445, Training Accuracy: 0.6069\n",
            "Epoch [1540/10000], Loss: 0.6445, Training Accuracy: 0.6073\n",
            "Epoch [1550/10000], Loss: 0.6445, Training Accuracy: 0.6085\n",
            "Epoch [1560/10000], Loss: 0.6478, Training Accuracy: 0.6054\n",
            "Epoch [1570/10000], Loss: 0.6451, Training Accuracy: 0.6079\n",
            "Epoch [1580/10000], Loss: 0.6449, Training Accuracy: 0.6082\n",
            "Epoch [1590/10000], Loss: 0.6445, Training Accuracy: 0.6085\n",
            "Epoch [1600/10000], Loss: 0.6444, Training Accuracy: 0.6107\n",
            "Epoch [1610/10000], Loss: 0.6443, Training Accuracy: 0.6076\n",
            "Epoch [1620/10000], Loss: 0.6443, Training Accuracy: 0.6069\n",
            "Epoch [1630/10000], Loss: 0.6443, Training Accuracy: 0.6076\n",
            "Epoch [1640/10000], Loss: 0.6442, Training Accuracy: 0.6076\n",
            "Epoch [1650/10000], Loss: 0.6442, Training Accuracy: 0.6079\n",
            "Epoch [1660/10000], Loss: 0.6442, Training Accuracy: 0.6076\n",
            "Epoch [1670/10000], Loss: 0.6441, Training Accuracy: 0.6076\n",
            "Epoch [1680/10000], Loss: 0.6441, Training Accuracy: 0.6076\n",
            "Epoch [1690/10000], Loss: 0.6441, Training Accuracy: 0.6079\n",
            "Epoch [1700/10000], Loss: 0.6441, Training Accuracy: 0.6073\n",
            "Epoch [1710/10000], Loss: 0.6440, Training Accuracy: 0.6073\n",
            "Epoch [1720/10000], Loss: 0.6440, Training Accuracy: 0.6079\n",
            "Epoch [1730/10000], Loss: 0.6440, Training Accuracy: 0.6076\n",
            "Epoch [1740/10000], Loss: 0.6439, Training Accuracy: 0.6076\n",
            "Epoch [1750/10000], Loss: 0.6439, Training Accuracy: 0.6098\n",
            "Epoch [1760/10000], Loss: 0.6509, Training Accuracy: 0.6050\n",
            "Epoch [1770/10000], Loss: 0.6452, Training Accuracy: 0.6123\n",
            "Epoch [1780/10000], Loss: 0.6444, Training Accuracy: 0.6136\n",
            "Epoch [1790/10000], Loss: 0.6438, Training Accuracy: 0.6076\n",
            "Epoch [1800/10000], Loss: 0.6438, Training Accuracy: 0.6085\n",
            "Epoch [1810/10000], Loss: 0.6437, Training Accuracy: 0.6073\n",
            "Epoch [1820/10000], Loss: 0.6437, Training Accuracy: 0.6069\n",
            "Epoch [1830/10000], Loss: 0.6436, Training Accuracy: 0.6073\n",
            "Epoch [1840/10000], Loss: 0.6436, Training Accuracy: 0.6082\n",
            "Epoch [1850/10000], Loss: 0.6436, Training Accuracy: 0.6082\n",
            "Epoch [1860/10000], Loss: 0.6435, Training Accuracy: 0.6082\n",
            "Epoch [1870/10000], Loss: 0.6435, Training Accuracy: 0.6085\n",
            "Epoch [1880/10000], Loss: 0.6435, Training Accuracy: 0.6085\n",
            "Epoch [1890/10000], Loss: 0.6435, Training Accuracy: 0.6085\n",
            "Epoch [1900/10000], Loss: 0.6434, Training Accuracy: 0.6091\n",
            "Epoch [1910/10000], Loss: 0.6434, Training Accuracy: 0.6101\n",
            "Epoch [1920/10000], Loss: 0.6472, Training Accuracy: 0.6038\n",
            "Epoch [1930/10000], Loss: 0.6444, Training Accuracy: 0.6082\n",
            "Epoch [1940/10000], Loss: 0.6436, Training Accuracy: 0.6079\n",
            "Epoch [1950/10000], Loss: 0.6434, Training Accuracy: 0.6079\n",
            "Epoch [1960/10000], Loss: 0.6433, Training Accuracy: 0.6079\n",
            "Epoch [1970/10000], Loss: 0.6432, Training Accuracy: 0.6085\n",
            "Epoch [1980/10000], Loss: 0.6432, Training Accuracy: 0.6091\n",
            "Epoch [1990/10000], Loss: 0.6431, Training Accuracy: 0.6095\n",
            "Epoch [2000/10000], Loss: 0.6431, Training Accuracy: 0.6085\n",
            "Epoch [2010/10000], Loss: 0.6431, Training Accuracy: 0.6085\n",
            "Epoch [2020/10000], Loss: 0.6430, Training Accuracy: 0.6079\n",
            "Epoch [2030/10000], Loss: 0.6430, Training Accuracy: 0.6082\n",
            "Epoch [2040/10000], Loss: 0.6430, Training Accuracy: 0.6091\n",
            "Epoch [2050/10000], Loss: 0.6430, Training Accuracy: 0.6098\n",
            "Epoch [2060/10000], Loss: 0.6469, Training Accuracy: 0.6057\n",
            "Epoch [2070/10000], Loss: 0.6437, Training Accuracy: 0.6110\n",
            "Epoch [2080/10000], Loss: 0.6434, Training Accuracy: 0.6129\n",
            "Epoch [2090/10000], Loss: 0.6429, Training Accuracy: 0.6126\n",
            "Epoch [2100/10000], Loss: 0.6428, Training Accuracy: 0.6085\n",
            "Epoch [2110/10000], Loss: 0.6428, Training Accuracy: 0.6085\n",
            "Epoch [2120/10000], Loss: 0.6427, Training Accuracy: 0.6088\n",
            "Epoch [2130/10000], Loss: 0.6427, Training Accuracy: 0.6088\n",
            "Epoch [2140/10000], Loss: 0.6427, Training Accuracy: 0.6088\n",
            "Epoch [2150/10000], Loss: 0.6426, Training Accuracy: 0.6095\n",
            "Epoch [2160/10000], Loss: 0.6426, Training Accuracy: 0.6110\n",
            "Epoch [2170/10000], Loss: 0.6452, Training Accuracy: 0.6079\n",
            "Epoch [2180/10000], Loss: 0.6442, Training Accuracy: 0.6104\n",
            "Epoch [2190/10000], Loss: 0.6425, Training Accuracy: 0.6085\n",
            "Epoch [2200/10000], Loss: 0.6427, Training Accuracy: 0.6091\n",
            "Epoch [2210/10000], Loss: 0.6425, Training Accuracy: 0.6104\n",
            "Epoch [2220/10000], Loss: 0.6424, Training Accuracy: 0.6085\n",
            "Epoch [2230/10000], Loss: 0.6424, Training Accuracy: 0.6085\n",
            "Epoch [2240/10000], Loss: 0.6424, Training Accuracy: 0.6088\n",
            "Epoch [2250/10000], Loss: 0.6424, Training Accuracy: 0.6101\n",
            "Epoch [2260/10000], Loss: 0.6432, Training Accuracy: 0.6088\n",
            "Epoch [2270/10000], Loss: 0.6423, Training Accuracy: 0.6088\n",
            "Epoch [2280/10000], Loss: 0.6423, Training Accuracy: 0.6104\n",
            "Epoch [2290/10000], Loss: 0.6422, Training Accuracy: 0.6126\n",
            "Epoch [2300/10000], Loss: 0.6422, Training Accuracy: 0.6120\n",
            "Epoch [2310/10000], Loss: 0.6423, Training Accuracy: 0.6095\n",
            "Epoch [2320/10000], Loss: 0.6421, Training Accuracy: 0.6126\n",
            "Epoch [2330/10000], Loss: 0.6427, Training Accuracy: 0.6095\n",
            "Epoch [2340/10000], Loss: 0.6421, Training Accuracy: 0.6129\n",
            "Epoch [2350/10000], Loss: 0.6420, Training Accuracy: 0.6098\n",
            "Epoch [2360/10000], Loss: 0.6436, Training Accuracy: 0.6091\n",
            "Epoch [2370/10000], Loss: 0.6423, Training Accuracy: 0.6091\n",
            "Epoch [2380/10000], Loss: 0.6421, Training Accuracy: 0.6107\n",
            "Epoch [2390/10000], Loss: 0.6423, Training Accuracy: 0.6098\n",
            "Epoch [2400/10000], Loss: 0.6458, Training Accuracy: 0.6060\n",
            "Epoch [2410/10000], Loss: 0.6418, Training Accuracy: 0.6110\n",
            "Epoch [2420/10000], Loss: 0.6421, Training Accuracy: 0.6101\n",
            "Epoch [2430/10000], Loss: 0.6417, Training Accuracy: 0.6110\n",
            "Epoch [2440/10000], Loss: 0.6417, Training Accuracy: 0.6117\n",
            "Epoch [2450/10000], Loss: 0.6417, Training Accuracy: 0.6104\n",
            "Epoch [2460/10000], Loss: 0.6416, Training Accuracy: 0.6107\n",
            "Epoch [2470/10000], Loss: 0.6417, Training Accuracy: 0.6101\n",
            "Epoch [2480/10000], Loss: 0.6425, Training Accuracy: 0.6098\n",
            "Epoch [2490/10000], Loss: 0.6416, Training Accuracy: 0.6114\n",
            "Epoch [2500/10000], Loss: 0.6416, Training Accuracy: 0.6117\n",
            "Epoch [2510/10000], Loss: 0.6415, Training Accuracy: 0.6120\n",
            "Epoch [2520/10000], Loss: 0.6433, Training Accuracy: 0.6104\n",
            "Epoch [2530/10000], Loss: 0.6429, Training Accuracy: 0.6079\n",
            "Epoch [2540/10000], Loss: 0.6426, Training Accuracy: 0.6104\n",
            "Epoch [2550/10000], Loss: 0.6414, Training Accuracy: 0.6098\n",
            "Epoch [2560/10000], Loss: 0.6417, Training Accuracy: 0.6123\n",
            "Epoch [2570/10000], Loss: 0.6414, Training Accuracy: 0.6110\n",
            "Epoch [2580/10000], Loss: 0.6413, Training Accuracy: 0.6114\n",
            "Epoch [2590/10000], Loss: 0.6413, Training Accuracy: 0.6123\n",
            "Epoch [2600/10000], Loss: 0.6413, Training Accuracy: 0.6104\n",
            "Epoch [2610/10000], Loss: 0.6414, Training Accuracy: 0.6120\n",
            "Epoch [2620/10000], Loss: 0.6419, Training Accuracy: 0.6076\n",
            "Epoch [2630/10000], Loss: 0.6412, Training Accuracy: 0.6145\n",
            "Epoch [2640/10000], Loss: 0.6414, Training Accuracy: 0.6088\n",
            "Epoch [2650/10000], Loss: 0.6416, Training Accuracy: 0.6104\n",
            "Epoch [2660/10000], Loss: 0.6411, Training Accuracy: 0.6136\n",
            "Epoch [2670/10000], Loss: 0.6423, Training Accuracy: 0.6095\n",
            "Epoch [2680/10000], Loss: 0.6410, Training Accuracy: 0.6114\n",
            "Epoch [2690/10000], Loss: 0.6411, Training Accuracy: 0.6107\n",
            "Epoch [2700/10000], Loss: 0.6412, Training Accuracy: 0.6091\n",
            "Epoch [2710/10000], Loss: 0.6412, Training Accuracy: 0.6107\n",
            "Epoch [2720/10000], Loss: 0.6411, Training Accuracy: 0.6126\n",
            "Epoch [2730/10000], Loss: 0.6426, Training Accuracy: 0.6098\n",
            "Epoch [2740/10000], Loss: 0.6421, Training Accuracy: 0.6073\n",
            "Epoch [2750/10000], Loss: 0.6418, Training Accuracy: 0.6107\n",
            "Epoch [2760/10000], Loss: 0.6417, Training Accuracy: 0.6117\n",
            "Epoch [2770/10000], Loss: 0.6416, Training Accuracy: 0.6101\n",
            "Epoch [2780/10000], Loss: 0.6408, Training Accuracy: 0.6117\n",
            "Epoch [2790/10000], Loss: 0.6407, Training Accuracy: 0.6148\n",
            "Epoch [2800/10000], Loss: 0.6407, Training Accuracy: 0.6120\n",
            "Epoch [2810/10000], Loss: 0.6407, Training Accuracy: 0.6114\n",
            "Epoch [2820/10000], Loss: 0.6406, Training Accuracy: 0.6136\n",
            "Epoch [2830/10000], Loss: 0.6407, Training Accuracy: 0.6139\n",
            "Epoch [2840/10000], Loss: 0.6428, Training Accuracy: 0.6041\n",
            "Epoch [2850/10000], Loss: 0.6410, Training Accuracy: 0.6110\n",
            "Epoch [2860/10000], Loss: 0.6407, Training Accuracy: 0.6117\n",
            "Epoch [2870/10000], Loss: 0.6405, Training Accuracy: 0.6126\n",
            "Epoch [2880/10000], Loss: 0.6408, Training Accuracy: 0.6101\n",
            "Epoch [2890/10000], Loss: 0.6409, Training Accuracy: 0.6107\n",
            "Epoch [2900/10000], Loss: 0.6405, Training Accuracy: 0.6129\n",
            "Epoch [2910/10000], Loss: 0.6403, Training Accuracy: 0.6139\n",
            "Epoch [2920/10000], Loss: 0.6410, Training Accuracy: 0.6104\n",
            "Epoch [2930/10000], Loss: 0.6404, Training Accuracy: 0.6110\n",
            "Epoch [2940/10000], Loss: 0.6412, Training Accuracy: 0.6114\n",
            "Epoch [2950/10000], Loss: 0.6406, Training Accuracy: 0.6098\n",
            "Epoch [2960/10000], Loss: 0.6402, Training Accuracy: 0.6123\n",
            "Epoch [2970/10000], Loss: 0.6402, Training Accuracy: 0.6145\n",
            "Epoch [2980/10000], Loss: 0.6402, Training Accuracy: 0.6145\n",
            "Epoch [2990/10000], Loss: 0.6402, Training Accuracy: 0.6136\n",
            "Epoch [3000/10000], Loss: 0.6403, Training Accuracy: 0.6129\n",
            "Epoch [3010/10000], Loss: 0.6401, Training Accuracy: 0.6139\n",
            "Epoch [3020/10000], Loss: 0.6401, Training Accuracy: 0.6142\n",
            "Epoch [3030/10000], Loss: 0.6439, Training Accuracy: 0.6076\n",
            "Epoch [3040/10000], Loss: 0.6407, Training Accuracy: 0.6120\n",
            "Epoch [3050/10000], Loss: 0.6409, Training Accuracy: 0.6088\n",
            "Epoch [3060/10000], Loss: 0.6399, Training Accuracy: 0.6126\n",
            "Epoch [3070/10000], Loss: 0.6400, Training Accuracy: 0.6139\n",
            "Epoch [3080/10000], Loss: 0.6399, Training Accuracy: 0.6120\n",
            "Epoch [3090/10000], Loss: 0.6398, Training Accuracy: 0.6145\n",
            "Epoch [3100/10000], Loss: 0.6398, Training Accuracy: 0.6139\n",
            "Epoch [3110/10000], Loss: 0.6397, Training Accuracy: 0.6145\n",
            "Epoch [3120/10000], Loss: 0.6399, Training Accuracy: 0.6132\n",
            "Epoch [3130/10000], Loss: 0.6405, Training Accuracy: 0.6139\n",
            "Epoch [3140/10000], Loss: 0.6409, Training Accuracy: 0.6114\n",
            "Epoch [3150/10000], Loss: 0.6402, Training Accuracy: 0.6114\n",
            "Epoch [3160/10000], Loss: 0.6397, Training Accuracy: 0.6132\n",
            "Epoch [3170/10000], Loss: 0.6396, Training Accuracy: 0.6145\n",
            "Epoch [3180/10000], Loss: 0.6397, Training Accuracy: 0.6120\n",
            "Epoch [3190/10000], Loss: 0.6395, Training Accuracy: 0.6132\n",
            "Epoch [3200/10000], Loss: 0.6399, Training Accuracy: 0.6110\n",
            "Epoch [3210/10000], Loss: 0.6401, Training Accuracy: 0.6126\n",
            "Epoch [3220/10000], Loss: 0.6394, Training Accuracy: 0.6136\n",
            "Epoch [3230/10000], Loss: 0.6400, Training Accuracy: 0.6104\n",
            "Epoch [3240/10000], Loss: 0.6397, Training Accuracy: 0.6136\n",
            "Epoch [3250/10000], Loss: 0.6393, Training Accuracy: 0.6126\n",
            "Epoch [3260/10000], Loss: 0.6392, Training Accuracy: 0.6132\n",
            "Epoch [3270/10000], Loss: 0.6392, Training Accuracy: 0.6158\n",
            "Epoch [3280/10000], Loss: 0.6392, Training Accuracy: 0.6167\n",
            "Epoch [3290/10000], Loss: 0.6393, Training Accuracy: 0.6132\n",
            "Epoch [3300/10000], Loss: 0.6405, Training Accuracy: 0.6104\n",
            "Epoch [3310/10000], Loss: 0.6391, Training Accuracy: 0.6142\n",
            "Epoch [3320/10000], Loss: 0.6394, Training Accuracy: 0.6145\n",
            "Epoch [3330/10000], Loss: 0.6391, Training Accuracy: 0.6129\n",
            "Epoch [3340/10000], Loss: 0.6393, Training Accuracy: 0.6117\n",
            "Epoch [3350/10000], Loss: 0.6391, Training Accuracy: 0.6132\n",
            "Epoch [3360/10000], Loss: 0.6394, Training Accuracy: 0.6098\n",
            "Epoch [3370/10000], Loss: 0.6394, Training Accuracy: 0.6114\n",
            "Epoch [3380/10000], Loss: 0.6387, Training Accuracy: 0.6158\n",
            "Epoch [3390/10000], Loss: 0.6391, Training Accuracy: 0.6142\n",
            "Epoch [3400/10000], Loss: 0.6390, Training Accuracy: 0.6139\n",
            "Epoch [3410/10000], Loss: 0.6392, Training Accuracy: 0.6139\n",
            "Epoch [3420/10000], Loss: 0.6394, Training Accuracy: 0.6145\n",
            "Epoch [3430/10000], Loss: 0.6387, Training Accuracy: 0.6132\n",
            "Epoch [3440/10000], Loss: 0.6395, Training Accuracy: 0.6098\n",
            "Epoch [3450/10000], Loss: 0.6385, Training Accuracy: 0.6155\n",
            "Epoch [3460/10000], Loss: 0.6389, Training Accuracy: 0.6158\n",
            "Epoch [3470/10000], Loss: 0.6385, Training Accuracy: 0.6164\n",
            "Epoch [3480/10000], Loss: 0.6383, Training Accuracy: 0.6142\n",
            "Epoch [3490/10000], Loss: 0.6384, Training Accuracy: 0.6148\n",
            "Epoch [3500/10000], Loss: 0.6423, Training Accuracy: 0.6050\n",
            "Epoch [3510/10000], Loss: 0.6385, Training Accuracy: 0.6117\n",
            "Epoch [3520/10000], Loss: 0.6384, Training Accuracy: 0.6161\n",
            "Epoch [3530/10000], Loss: 0.6383, Training Accuracy: 0.6132\n",
            "Epoch [3540/10000], Loss: 0.6381, Training Accuracy: 0.6174\n",
            "Epoch [3550/10000], Loss: 0.6380, Training Accuracy: 0.6164\n",
            "Epoch [3560/10000], Loss: 0.6380, Training Accuracy: 0.6148\n",
            "Epoch [3570/10000], Loss: 0.6379, Training Accuracy: 0.6155\n",
            "Epoch [3580/10000], Loss: 0.6379, Training Accuracy: 0.6145\n",
            "Epoch [3590/10000], Loss: 0.6382, Training Accuracy: 0.6136\n",
            "Epoch [3600/10000], Loss: 0.6400, Training Accuracy: 0.6110\n",
            "Epoch [3610/10000], Loss: 0.6392, Training Accuracy: 0.6098\n",
            "Epoch [3620/10000], Loss: 0.6387, Training Accuracy: 0.6139\n",
            "Epoch [3630/10000], Loss: 0.6382, Training Accuracy: 0.6104\n",
            "Epoch [3640/10000], Loss: 0.6380, Training Accuracy: 0.6164\n",
            "Epoch [3650/10000], Loss: 0.6376, Training Accuracy: 0.6161\n",
            "Epoch [3660/10000], Loss: 0.6378, Training Accuracy: 0.6151\n",
            "Epoch [3670/10000], Loss: 0.6378, Training Accuracy: 0.6161\n",
            "Epoch [3680/10000], Loss: 0.6384, Training Accuracy: 0.6095\n",
            "Epoch [3690/10000], Loss: 0.6378, Training Accuracy: 0.6164\n",
            "Epoch [3700/10000], Loss: 0.6380, Training Accuracy: 0.6177\n",
            "Epoch [3710/10000], Loss: 0.6375, Training Accuracy: 0.6170\n",
            "Epoch [3720/10000], Loss: 0.6374, Training Accuracy: 0.6161\n",
            "Epoch [3730/10000], Loss: 0.6383, Training Accuracy: 0.6095\n",
            "Epoch [3740/10000], Loss: 0.6374, Training Accuracy: 0.6151\n",
            "Epoch [3750/10000], Loss: 0.6383, Training Accuracy: 0.6101\n",
            "Epoch [3760/10000], Loss: 0.6375, Training Accuracy: 0.6167\n",
            "Epoch [3770/10000], Loss: 0.6373, Training Accuracy: 0.6151\n",
            "Epoch [3780/10000], Loss: 0.6372, Training Accuracy: 0.6183\n",
            "Epoch [3790/10000], Loss: 0.6371, Training Accuracy: 0.6174\n",
            "Epoch [3800/10000], Loss: 0.6371, Training Accuracy: 0.6186\n",
            "Epoch [3810/10000], Loss: 0.6370, Training Accuracy: 0.6177\n",
            "Epoch [3820/10000], Loss: 0.6370, Training Accuracy: 0.6180\n",
            "Epoch [3830/10000], Loss: 0.6371, Training Accuracy: 0.6167\n",
            "Epoch [3840/10000], Loss: 0.6424, Training Accuracy: 0.6110\n",
            "Epoch [3850/10000], Loss: 0.6378, Training Accuracy: 0.6142\n",
            "Epoch [3860/10000], Loss: 0.6374, Training Accuracy: 0.6158\n",
            "Epoch [3870/10000], Loss: 0.6371, Training Accuracy: 0.6189\n",
            "Epoch [3880/10000], Loss: 0.6368, Training Accuracy: 0.6180\n",
            "Epoch [3890/10000], Loss: 0.6367, Training Accuracy: 0.6177\n",
            "Epoch [3900/10000], Loss: 0.6367, Training Accuracy: 0.6177\n",
            "Epoch [3910/10000], Loss: 0.6367, Training Accuracy: 0.6164\n",
            "Epoch [3920/10000], Loss: 0.6368, Training Accuracy: 0.6170\n",
            "Epoch [3930/10000], Loss: 0.6393, Training Accuracy: 0.6117\n",
            "Epoch [3940/10000], Loss: 0.6374, Training Accuracy: 0.6136\n",
            "Epoch [3950/10000], Loss: 0.6365, Training Accuracy: 0.6164\n",
            "Epoch [3960/10000], Loss: 0.6369, Training Accuracy: 0.6151\n",
            "Epoch [3970/10000], Loss: 0.6365, Training Accuracy: 0.6158\n",
            "Epoch [3980/10000], Loss: 0.6370, Training Accuracy: 0.6139\n",
            "Epoch [3990/10000], Loss: 0.6375, Training Accuracy: 0.6142\n",
            "Epoch [4000/10000], Loss: 0.6369, Training Accuracy: 0.6174\n",
            "Epoch [4010/10000], Loss: 0.6365, Training Accuracy: 0.6183\n",
            "Epoch [4020/10000], Loss: 0.6367, Training Accuracy: 0.6183\n",
            "Epoch [4030/10000], Loss: 0.6367, Training Accuracy: 0.6139\n",
            "Epoch [4040/10000], Loss: 0.6368, Training Accuracy: 0.6139\n",
            "Epoch [4050/10000], Loss: 0.6371, Training Accuracy: 0.6142\n",
            "Epoch [4060/10000], Loss: 0.6361, Training Accuracy: 0.6186\n",
            "Epoch [4070/10000], Loss: 0.6366, Training Accuracy: 0.6164\n",
            "Epoch [4080/10000], Loss: 0.6382, Training Accuracy: 0.6120\n",
            "Epoch [4090/10000], Loss: 0.6368, Training Accuracy: 0.6142\n",
            "Epoch [4100/10000], Loss: 0.6360, Training Accuracy: 0.6196\n",
            "Epoch [4110/10000], Loss: 0.6363, Training Accuracy: 0.6196\n",
            "Epoch [4120/10000], Loss: 0.6360, Training Accuracy: 0.6192\n",
            "Epoch [4130/10000], Loss: 0.6363, Training Accuracy: 0.6155\n",
            "Epoch [4140/10000], Loss: 0.6387, Training Accuracy: 0.6120\n",
            "Epoch [4150/10000], Loss: 0.6371, Training Accuracy: 0.6145\n",
            "Epoch [4160/10000], Loss: 0.6360, Training Accuracy: 0.6136\n",
            "Epoch [4170/10000], Loss: 0.6359, Training Accuracy: 0.6192\n",
            "Epoch [4180/10000], Loss: 0.6358, Training Accuracy: 0.6174\n",
            "Epoch [4190/10000], Loss: 0.6357, Training Accuracy: 0.6177\n",
            "Epoch [4200/10000], Loss: 0.6358, Training Accuracy: 0.6192\n",
            "Epoch [4210/10000], Loss: 0.6362, Training Accuracy: 0.6145\n",
            "Epoch [4220/10000], Loss: 0.6378, Training Accuracy: 0.6132\n",
            "Epoch [4230/10000], Loss: 0.6365, Training Accuracy: 0.6129\n",
            "Epoch [4240/10000], Loss: 0.6356, Training Accuracy: 0.6208\n",
            "Epoch [4250/10000], Loss: 0.6359, Training Accuracy: 0.6199\n",
            "Epoch [4260/10000], Loss: 0.6356, Training Accuracy: 0.6205\n",
            "Epoch [4270/10000], Loss: 0.6358, Training Accuracy: 0.6186\n",
            "Epoch [4280/10000], Loss: 0.6381, Training Accuracy: 0.6126\n",
            "Epoch [4290/10000], Loss: 0.6358, Training Accuracy: 0.6148\n",
            "Epoch [4300/10000], Loss: 0.6353, Training Accuracy: 0.6202\n",
            "Epoch [4310/10000], Loss: 0.6353, Training Accuracy: 0.6196\n",
            "Epoch [4320/10000], Loss: 0.6354, Training Accuracy: 0.6189\n",
            "Epoch [4330/10000], Loss: 0.6362, Training Accuracy: 0.6088\n",
            "Epoch [4340/10000], Loss: 0.6359, Training Accuracy: 0.6142\n",
            "Epoch [4350/10000], Loss: 0.6356, Training Accuracy: 0.6174\n",
            "Epoch [4360/10000], Loss: 0.6356, Training Accuracy: 0.6199\n",
            "Epoch [4370/10000], Loss: 0.6352, Training Accuracy: 0.6189\n",
            "Epoch [4380/10000], Loss: 0.6352, Training Accuracy: 0.6199\n",
            "Epoch [4390/10000], Loss: 0.6394, Training Accuracy: 0.6123\n",
            "Epoch [4400/10000], Loss: 0.6373, Training Accuracy: 0.6114\n",
            "Epoch [4410/10000], Loss: 0.6355, Training Accuracy: 0.6215\n",
            "Epoch [4420/10000], Loss: 0.6349, Training Accuracy: 0.6196\n",
            "Epoch [4430/10000], Loss: 0.6349, Training Accuracy: 0.6174\n",
            "Epoch [4440/10000], Loss: 0.6349, Training Accuracy: 0.6174\n",
            "Epoch [4450/10000], Loss: 0.6348, Training Accuracy: 0.6174\n",
            "Epoch [4460/10000], Loss: 0.6348, Training Accuracy: 0.6167\n",
            "Epoch [4470/10000], Loss: 0.6371, Training Accuracy: 0.6167\n",
            "Epoch [4480/10000], Loss: 0.6357, Training Accuracy: 0.6142\n",
            "Epoch [4490/10000], Loss: 0.6360, Training Accuracy: 0.6136\n",
            "Epoch [4500/10000], Loss: 0.6347, Training Accuracy: 0.6180\n",
            "Epoch [4510/10000], Loss: 0.6347, Training Accuracy: 0.6224\n",
            "Epoch [4520/10000], Loss: 0.6347, Training Accuracy: 0.6186\n",
            "Epoch [4530/10000], Loss: 0.6346, Training Accuracy: 0.6211\n",
            "Epoch [4540/10000], Loss: 0.6346, Training Accuracy: 0.6196\n",
            "Epoch [4550/10000], Loss: 0.6345, Training Accuracy: 0.6186\n",
            "Epoch [4560/10000], Loss: 0.6346, Training Accuracy: 0.6155\n",
            "Epoch [4570/10000], Loss: 0.6409, Training Accuracy: 0.6110\n",
            "Epoch [4580/10000], Loss: 0.6372, Training Accuracy: 0.6155\n",
            "Epoch [4590/10000], Loss: 0.6344, Training Accuracy: 0.6199\n",
            "Epoch [4600/10000], Loss: 0.6347, Training Accuracy: 0.6167\n",
            "Epoch [4610/10000], Loss: 0.6344, Training Accuracy: 0.6202\n",
            "Epoch [4620/10000], Loss: 0.6343, Training Accuracy: 0.6192\n",
            "Epoch [4630/10000], Loss: 0.6343, Training Accuracy: 0.6196\n",
            "Epoch [4640/10000], Loss: 0.6343, Training Accuracy: 0.6199\n",
            "Epoch [4650/10000], Loss: 0.6342, Training Accuracy: 0.6189\n",
            "Epoch [4660/10000], Loss: 0.6351, Training Accuracy: 0.6114\n",
            "Epoch [4670/10000], Loss: 0.6358, Training Accuracy: 0.6126\n",
            "Epoch [4680/10000], Loss: 0.6343, Training Accuracy: 0.6170\n",
            "Epoch [4690/10000], Loss: 0.6342, Training Accuracy: 0.6208\n",
            "Epoch [4700/10000], Loss: 0.6342, Training Accuracy: 0.6161\n",
            "Epoch [4710/10000], Loss: 0.6341, Training Accuracy: 0.6211\n",
            "Epoch [4720/10000], Loss: 0.6340, Training Accuracy: 0.6192\n",
            "Epoch [4730/10000], Loss: 0.6341, Training Accuracy: 0.6155\n",
            "Epoch [4740/10000], Loss: 0.6346, Training Accuracy: 0.6132\n",
            "Epoch [4750/10000], Loss: 0.6363, Training Accuracy: 0.6164\n",
            "Epoch [4760/10000], Loss: 0.6343, Training Accuracy: 0.6180\n",
            "Epoch [4770/10000], Loss: 0.6340, Training Accuracy: 0.6189\n",
            "Epoch [4780/10000], Loss: 0.6342, Training Accuracy: 0.6139\n",
            "Epoch [4790/10000], Loss: 0.6338, Training Accuracy: 0.6205\n",
            "Epoch [4800/10000], Loss: 0.6339, Training Accuracy: 0.6174\n",
            "Epoch [4810/10000], Loss: 0.6381, Training Accuracy: 0.6145\n",
            "Epoch [4820/10000], Loss: 0.6358, Training Accuracy: 0.6151\n",
            "Epoch [4830/10000], Loss: 0.6349, Training Accuracy: 0.6136\n",
            "Epoch [4840/10000], Loss: 0.6340, Training Accuracy: 0.6218\n",
            "Epoch [4850/10000], Loss: 0.6337, Training Accuracy: 0.6224\n",
            "Epoch [4860/10000], Loss: 0.6337, Training Accuracy: 0.6243\n",
            "Epoch [4870/10000], Loss: 0.6337, Training Accuracy: 0.6183\n",
            "Epoch [4880/10000], Loss: 0.6336, Training Accuracy: 0.6192\n",
            "Epoch [4890/10000], Loss: 0.6343, Training Accuracy: 0.6155\n",
            "Epoch [4900/10000], Loss: 0.6360, Training Accuracy: 0.6180\n",
            "Epoch [4910/10000], Loss: 0.6340, Training Accuracy: 0.6177\n",
            "Epoch [4920/10000], Loss: 0.6335, Training Accuracy: 0.6205\n",
            "Epoch [4930/10000], Loss: 0.6339, Training Accuracy: 0.6142\n",
            "Epoch [4940/10000], Loss: 0.6336, Training Accuracy: 0.6164\n",
            "Epoch [4950/10000], Loss: 0.6338, Training Accuracy: 0.6132\n",
            "Epoch [4960/10000], Loss: 0.6365, Training Accuracy: 0.6174\n",
            "Epoch [4970/10000], Loss: 0.6339, Training Accuracy: 0.6186\n",
            "Epoch [4980/10000], Loss: 0.6334, Training Accuracy: 0.6151\n",
            "Epoch [4990/10000], Loss: 0.6334, Training Accuracy: 0.6218\n",
            "Epoch [5000/10000], Loss: 0.6336, Training Accuracy: 0.6202\n",
            "Epoch [5010/10000], Loss: 0.6337, Training Accuracy: 0.6199\n",
            "Epoch [5020/10000], Loss: 0.6337, Training Accuracy: 0.6205\n",
            "Epoch [5030/10000], Loss: 0.6344, Training Accuracy: 0.6164\n",
            "Epoch [5040/10000], Loss: 0.6336, Training Accuracy: 0.6230\n",
            "Epoch [5050/10000], Loss: 0.6335, Training Accuracy: 0.6136\n",
            "Epoch [5060/10000], Loss: 0.6349, Training Accuracy: 0.6142\n",
            "Epoch [5070/10000], Loss: 0.6331, Training Accuracy: 0.6202\n",
            "Epoch [5080/10000], Loss: 0.6335, Training Accuracy: 0.6205\n",
            "Epoch [5090/10000], Loss: 0.6332, Training Accuracy: 0.6227\n",
            "Epoch [5100/10000], Loss: 0.6337, Training Accuracy: 0.6189\n",
            "Epoch [5110/10000], Loss: 0.6367, Training Accuracy: 0.6139\n",
            "Epoch [5120/10000], Loss: 0.6341, Training Accuracy: 0.6155\n",
            "Epoch [5130/10000], Loss: 0.6332, Training Accuracy: 0.6227\n",
            "Epoch [5140/10000], Loss: 0.6331, Training Accuracy: 0.6183\n",
            "Epoch [5150/10000], Loss: 0.6330, Training Accuracy: 0.6233\n",
            "Epoch [5160/10000], Loss: 0.6331, Training Accuracy: 0.6208\n",
            "Epoch [5170/10000], Loss: 0.6334, Training Accuracy: 0.6202\n",
            "Epoch [5180/10000], Loss: 0.6338, Training Accuracy: 0.6177\n",
            "Epoch [5190/10000], Loss: 0.6328, Training Accuracy: 0.6208\n",
            "Epoch [5200/10000], Loss: 0.6327, Training Accuracy: 0.6205\n",
            "Epoch [5210/10000], Loss: 0.6347, Training Accuracy: 0.6142\n",
            "Epoch [5220/10000], Loss: 0.6326, Training Accuracy: 0.6145\n",
            "Epoch [5230/10000], Loss: 0.6336, Training Accuracy: 0.6174\n",
            "Epoch [5240/10000], Loss: 0.6330, Training Accuracy: 0.6174\n",
            "Epoch [5250/10000], Loss: 0.6325, Training Accuracy: 0.6199\n",
            "Epoch [5260/10000], Loss: 0.6325, Training Accuracy: 0.6192\n",
            "Epoch [5270/10000], Loss: 0.6325, Training Accuracy: 0.6205\n",
            "Epoch [5280/10000], Loss: 0.6324, Training Accuracy: 0.6196\n",
            "Epoch [5290/10000], Loss: 0.6327, Training Accuracy: 0.6151\n",
            "Epoch [5300/10000], Loss: 0.6333, Training Accuracy: 0.6132\n",
            "Epoch [5310/10000], Loss: 0.6329, Training Accuracy: 0.6148\n",
            "Epoch [5320/10000], Loss: 0.6323, Training Accuracy: 0.6202\n",
            "Epoch [5330/10000], Loss: 0.6331, Training Accuracy: 0.6164\n",
            "Epoch [5340/10000], Loss: 0.6351, Training Accuracy: 0.6183\n",
            "Epoch [5350/10000], Loss: 0.6333, Training Accuracy: 0.6208\n",
            "Epoch [5360/10000], Loss: 0.6328, Training Accuracy: 0.6164\n",
            "Epoch [5370/10000], Loss: 0.6324, Training Accuracy: 0.6215\n",
            "Epoch [5380/10000], Loss: 0.6323, Training Accuracy: 0.6192\n",
            "Epoch [5390/10000], Loss: 0.6322, Training Accuracy: 0.6208\n",
            "Epoch [5400/10000], Loss: 0.6338, Training Accuracy: 0.6177\n",
            "Epoch [5410/10000], Loss: 0.6322, Training Accuracy: 0.6192\n",
            "Epoch [5420/10000], Loss: 0.6330, Training Accuracy: 0.6177\n",
            "Epoch [5430/10000], Loss: 0.6324, Training Accuracy: 0.6167\n",
            "Epoch [5440/10000], Loss: 0.6320, Training Accuracy: 0.6192\n",
            "Epoch [5450/10000], Loss: 0.6331, Training Accuracy: 0.6180\n",
            "Epoch [5460/10000], Loss: 0.6335, Training Accuracy: 0.6183\n",
            "Epoch [5470/10000], Loss: 0.6329, Training Accuracy: 0.6170\n",
            "Epoch [5480/10000], Loss: 0.6323, Training Accuracy: 0.6196\n",
            "Epoch [5490/10000], Loss: 0.6321, Training Accuracy: 0.6170\n",
            "Epoch [5500/10000], Loss: 0.6318, Training Accuracy: 0.6183\n",
            "Epoch [5510/10000], Loss: 0.6319, Training Accuracy: 0.6202\n",
            "Epoch [5520/10000], Loss: 0.6336, Training Accuracy: 0.6155\n",
            "Epoch [5530/10000], Loss: 0.6320, Training Accuracy: 0.6189\n",
            "Epoch [5540/10000], Loss: 0.6329, Training Accuracy: 0.6145\n",
            "Epoch [5550/10000], Loss: 0.6317, Training Accuracy: 0.6192\n",
            "Epoch [5560/10000], Loss: 0.6324, Training Accuracy: 0.6202\n",
            "Epoch [5570/10000], Loss: 0.6321, Training Accuracy: 0.6208\n",
            "Epoch [5580/10000], Loss: 0.6320, Training Accuracy: 0.6205\n",
            "Epoch [5590/10000], Loss: 0.6337, Training Accuracy: 0.6155\n",
            "Epoch [5600/10000], Loss: 0.6316, Training Accuracy: 0.6205\n",
            "Epoch [5610/10000], Loss: 0.6324, Training Accuracy: 0.6161\n",
            "Epoch [5620/10000], Loss: 0.6316, Training Accuracy: 0.6189\n",
            "Epoch [5630/10000], Loss: 0.6315, Training Accuracy: 0.6192\n",
            "Epoch [5640/10000], Loss: 0.6322, Training Accuracy: 0.6183\n",
            "Epoch [5650/10000], Loss: 0.6349, Training Accuracy: 0.6205\n",
            "Epoch [5660/10000], Loss: 0.6319, Training Accuracy: 0.6174\n",
            "Epoch [5670/10000], Loss: 0.6315, Training Accuracy: 0.6192\n",
            "Epoch [5680/10000], Loss: 0.6315, Training Accuracy: 0.6177\n",
            "Epoch [5690/10000], Loss: 0.6315, Training Accuracy: 0.6199\n",
            "Epoch [5700/10000], Loss: 0.6314, Training Accuracy: 0.6186\n",
            "Epoch [5710/10000], Loss: 0.6315, Training Accuracy: 0.6183\n",
            "Epoch [5720/10000], Loss: 0.6313, Training Accuracy: 0.6196\n",
            "Epoch [5730/10000], Loss: 0.6314, Training Accuracy: 0.6192\n",
            "Epoch [5740/10000], Loss: 0.6346, Training Accuracy: 0.6211\n",
            "Epoch [5750/10000], Loss: 0.6315, Training Accuracy: 0.6227\n",
            "Epoch [5760/10000], Loss: 0.6312, Training Accuracy: 0.6180\n",
            "Epoch [5770/10000], Loss: 0.6314, Training Accuracy: 0.6192\n",
            "Epoch [5780/10000], Loss: 0.6314, Training Accuracy: 0.6237\n",
            "Epoch [5790/10000], Loss: 0.6312, Training Accuracy: 0.6202\n",
            "Epoch [5800/10000], Loss: 0.6314, Training Accuracy: 0.6224\n",
            "Epoch [5810/10000], Loss: 0.6351, Training Accuracy: 0.6161\n",
            "Epoch [5820/10000], Loss: 0.6325, Training Accuracy: 0.6205\n",
            "Epoch [5830/10000], Loss: 0.6317, Training Accuracy: 0.6208\n",
            "Epoch [5840/10000], Loss: 0.6312, Training Accuracy: 0.6189\n",
            "Epoch [5850/10000], Loss: 0.6311, Training Accuracy: 0.6189\n",
            "Epoch [5860/10000], Loss: 0.6310, Training Accuracy: 0.6161\n",
            "Epoch [5870/10000], Loss: 0.6315, Training Accuracy: 0.6196\n",
            "Epoch [5880/10000], Loss: 0.6332, Training Accuracy: 0.6183\n",
            "Epoch [5890/10000], Loss: 0.6322, Training Accuracy: 0.6227\n",
            "Epoch [5900/10000], Loss: 0.6310, Training Accuracy: 0.6218\n",
            "Epoch [5910/10000], Loss: 0.6314, Training Accuracy: 0.6218\n",
            "Epoch [5920/10000], Loss: 0.6309, Training Accuracy: 0.6170\n",
            "Epoch [5930/10000], Loss: 0.6310, Training Accuracy: 0.6221\n",
            "Epoch [5940/10000], Loss: 0.6339, Training Accuracy: 0.6199\n",
            "Epoch [5950/10000], Loss: 0.6313, Training Accuracy: 0.6199\n",
            "Epoch [5960/10000], Loss: 0.6308, Training Accuracy: 0.6215\n",
            "Epoch [5970/10000], Loss: 0.6312, Training Accuracy: 0.6227\n",
            "Epoch [5980/10000], Loss: 0.6307, Training Accuracy: 0.6202\n",
            "Epoch [5990/10000], Loss: 0.6318, Training Accuracy: 0.6180\n",
            "Epoch [6000/10000], Loss: 0.6312, Training Accuracy: 0.6215\n",
            "Epoch [6010/10000], Loss: 0.6307, Training Accuracy: 0.6164\n",
            "Epoch [6020/10000], Loss: 0.6319, Training Accuracy: 0.6230\n",
            "Epoch [6030/10000], Loss: 0.6313, Training Accuracy: 0.6215\n",
            "Epoch [6040/10000], Loss: 0.6313, Training Accuracy: 0.6199\n",
            "Epoch [6050/10000], Loss: 0.6305, Training Accuracy: 0.6186\n",
            "Epoch [6060/10000], Loss: 0.6311, Training Accuracy: 0.6208\n",
            "Epoch [6070/10000], Loss: 0.6317, Training Accuracy: 0.6211\n",
            "Epoch [6080/10000], Loss: 0.6306, Training Accuracy: 0.6183\n",
            "Epoch [6090/10000], Loss: 0.6305, Training Accuracy: 0.6208\n",
            "Epoch [6100/10000], Loss: 0.6332, Training Accuracy: 0.6208\n",
            "Epoch [6110/10000], Loss: 0.6304, Training Accuracy: 0.6205\n",
            "Epoch [6120/10000], Loss: 0.6304, Training Accuracy: 0.6196\n",
            "Epoch [6130/10000], Loss: 0.6305, Training Accuracy: 0.6199\n",
            "Epoch [6140/10000], Loss: 0.6305, Training Accuracy: 0.6199\n",
            "Epoch [6150/10000], Loss: 0.6305, Training Accuracy: 0.6218\n",
            "Epoch [6160/10000], Loss: 0.6311, Training Accuracy: 0.6208\n",
            "Epoch [6170/10000], Loss: 0.6313, Training Accuracy: 0.6208\n",
            "Epoch [6180/10000], Loss: 0.6304, Training Accuracy: 0.6202\n",
            "Epoch [6190/10000], Loss: 0.6303, Training Accuracy: 0.6211\n",
            "Epoch [6200/10000], Loss: 0.6330, Training Accuracy: 0.6164\n",
            "Epoch [6210/10000], Loss: 0.6302, Training Accuracy: 0.6249\n",
            "Epoch [6220/10000], Loss: 0.6301, Training Accuracy: 0.6199\n",
            "Epoch [6230/10000], Loss: 0.6302, Training Accuracy: 0.6205\n",
            "Epoch [6240/10000], Loss: 0.6303, Training Accuracy: 0.6218\n",
            "Epoch [6250/10000], Loss: 0.6301, Training Accuracy: 0.6218\n",
            "Epoch [6260/10000], Loss: 0.6304, Training Accuracy: 0.6221\n",
            "Epoch [6270/10000], Loss: 0.6332, Training Accuracy: 0.6170\n",
            "Epoch [6280/10000], Loss: 0.6306, Training Accuracy: 0.6240\n",
            "Epoch [6290/10000], Loss: 0.6300, Training Accuracy: 0.6221\n",
            "Epoch [6300/10000], Loss: 0.6306, Training Accuracy: 0.6218\n",
            "Epoch [6310/10000], Loss: 0.6299, Training Accuracy: 0.6189\n",
            "Epoch [6320/10000], Loss: 0.6310, Training Accuracy: 0.6243\n",
            "Epoch [6330/10000], Loss: 0.6302, Training Accuracy: 0.6215\n",
            "Epoch [6340/10000], Loss: 0.6299, Training Accuracy: 0.6224\n",
            "Epoch [6350/10000], Loss: 0.6304, Training Accuracy: 0.6211\n",
            "Epoch [6360/10000], Loss: 0.6340, Training Accuracy: 0.6161\n",
            "Epoch [6370/10000], Loss: 0.6318, Training Accuracy: 0.6170\n",
            "Epoch [6380/10000], Loss: 0.6306, Training Accuracy: 0.6218\n",
            "Epoch [6390/10000], Loss: 0.6301, Training Accuracy: 0.6252\n",
            "Epoch [6400/10000], Loss: 0.6298, Training Accuracy: 0.6243\n",
            "Epoch [6410/10000], Loss: 0.6299, Training Accuracy: 0.6215\n",
            "Epoch [6420/10000], Loss: 0.6303, Training Accuracy: 0.6208\n",
            "Epoch [6430/10000], Loss: 0.6312, Training Accuracy: 0.6199\n",
            "Epoch [6440/10000], Loss: 0.6297, Training Accuracy: 0.6221\n",
            "Epoch [6450/10000], Loss: 0.6306, Training Accuracy: 0.6177\n",
            "Epoch [6460/10000], Loss: 0.6301, Training Accuracy: 0.6215\n",
            "Epoch [6470/10000], Loss: 0.6296, Training Accuracy: 0.6224\n",
            "Epoch [6480/10000], Loss: 0.6298, Training Accuracy: 0.6237\n",
            "Epoch [6490/10000], Loss: 0.6297, Training Accuracy: 0.6211\n",
            "Epoch [6500/10000], Loss: 0.6296, Training Accuracy: 0.6259\n",
            "Epoch [6510/10000], Loss: 0.6296, Training Accuracy: 0.6246\n",
            "Epoch [6520/10000], Loss: 0.6295, Training Accuracy: 0.6221\n",
            "Epoch [6530/10000], Loss: 0.6295, Training Accuracy: 0.6265\n",
            "Epoch [6540/10000], Loss: 0.6307, Training Accuracy: 0.6196\n",
            "Epoch [6550/10000], Loss: 0.6307, Training Accuracy: 0.6246\n",
            "Epoch [6560/10000], Loss: 0.6317, Training Accuracy: 0.6249\n",
            "Epoch [6570/10000], Loss: 0.6300, Training Accuracy: 0.6233\n",
            "Epoch [6580/10000], Loss: 0.6297, Training Accuracy: 0.6205\n",
            "Epoch [6590/10000], Loss: 0.6296, Training Accuracy: 0.6230\n",
            "Epoch [6600/10000], Loss: 0.6294, Training Accuracy: 0.6218\n",
            "Epoch [6610/10000], Loss: 0.6295, Training Accuracy: 0.6215\n",
            "Epoch [6620/10000], Loss: 0.6300, Training Accuracy: 0.6233\n",
            "Epoch [6630/10000], Loss: 0.6305, Training Accuracy: 0.6256\n",
            "Epoch [6640/10000], Loss: 0.6293, Training Accuracy: 0.6252\n",
            "Epoch [6650/10000], Loss: 0.6294, Training Accuracy: 0.6215\n",
            "Epoch [6660/10000], Loss: 0.6321, Training Accuracy: 0.6170\n",
            "Epoch [6670/10000], Loss: 0.6293, Training Accuracy: 0.6240\n",
            "Epoch [6680/10000], Loss: 0.6293, Training Accuracy: 0.6233\n",
            "Epoch [6690/10000], Loss: 0.6297, Training Accuracy: 0.6240\n",
            "Epoch [6700/10000], Loss: 0.6294, Training Accuracy: 0.6221\n",
            "Epoch [6710/10000], Loss: 0.6295, Training Accuracy: 0.6224\n",
            "Epoch [6720/10000], Loss: 0.6295, Training Accuracy: 0.6240\n",
            "Epoch [6730/10000], Loss: 0.6305, Training Accuracy: 0.6230\n",
            "Epoch [6740/10000], Loss: 0.6292, Training Accuracy: 0.6218\n",
            "Epoch [6750/10000], Loss: 0.6292, Training Accuracy: 0.6221\n",
            "Epoch [6760/10000], Loss: 0.6320, Training Accuracy: 0.6164\n",
            "Epoch [6770/10000], Loss: 0.6291, Training Accuracy: 0.6230\n",
            "Epoch [6780/10000], Loss: 0.6291, Training Accuracy: 0.6221\n",
            "Epoch [6790/10000], Loss: 0.6295, Training Accuracy: 0.6259\n",
            "Epoch [6800/10000], Loss: 0.6290, Training Accuracy: 0.6227\n",
            "Epoch [6810/10000], Loss: 0.6295, Training Accuracy: 0.6243\n",
            "Epoch [6820/10000], Loss: 0.6298, Training Accuracy: 0.6256\n",
            "Epoch [6830/10000], Loss: 0.6291, Training Accuracy: 0.6221\n",
            "Epoch [6840/10000], Loss: 0.6294, Training Accuracy: 0.6259\n",
            "Epoch [6850/10000], Loss: 0.6309, Training Accuracy: 0.6192\n",
            "Epoch [6860/10000], Loss: 0.6289, Training Accuracy: 0.6227\n",
            "Epoch [6870/10000], Loss: 0.6299, Training Accuracy: 0.6221\n",
            "Epoch [6880/10000], Loss: 0.6289, Training Accuracy: 0.6224\n",
            "Epoch [6890/10000], Loss: 0.6288, Training Accuracy: 0.6265\n",
            "Epoch [6900/10000], Loss: 0.6297, Training Accuracy: 0.6215\n",
            "Epoch [6910/10000], Loss: 0.6320, Training Accuracy: 0.6202\n",
            "Epoch [6920/10000], Loss: 0.6299, Training Accuracy: 0.6237\n",
            "Epoch [6930/10000], Loss: 0.6290, Training Accuracy: 0.6249\n",
            "Epoch [6940/10000], Loss: 0.6290, Training Accuracy: 0.6224\n",
            "Epoch [6950/10000], Loss: 0.6290, Training Accuracy: 0.6240\n",
            "Epoch [6960/10000], Loss: 0.6287, Training Accuracy: 0.6211\n",
            "Epoch [6970/10000], Loss: 0.6289, Training Accuracy: 0.6211\n",
            "Epoch [6980/10000], Loss: 0.6294, Training Accuracy: 0.6265\n",
            "Epoch [6990/10000], Loss: 0.6298, Training Accuracy: 0.6274\n",
            "Epoch [7000/10000], Loss: 0.6287, Training Accuracy: 0.6252\n",
            "Epoch [7010/10000], Loss: 0.6302, Training Accuracy: 0.6199\n",
            "Epoch [7020/10000], Loss: 0.6286, Training Accuracy: 0.6252\n",
            "Epoch [7030/10000], Loss: 0.6295, Training Accuracy: 0.6262\n",
            "Epoch [7040/10000], Loss: 0.6291, Training Accuracy: 0.6243\n",
            "Epoch [7050/10000], Loss: 0.6285, Training Accuracy: 0.6221\n",
            "Epoch [7060/10000], Loss: 0.6298, Training Accuracy: 0.6208\n",
            "Epoch [7070/10000], Loss: 0.6294, Training Accuracy: 0.6230\n",
            "Epoch [7080/10000], Loss: 0.6298, Training Accuracy: 0.6259\n",
            "Epoch [7090/10000], Loss: 0.6287, Training Accuracy: 0.6237\n",
            "Epoch [7100/10000], Loss: 0.6288, Training Accuracy: 0.6230\n",
            "Epoch [7110/10000], Loss: 0.6284, Training Accuracy: 0.6268\n",
            "Epoch [7120/10000], Loss: 0.6287, Training Accuracy: 0.6246\n",
            "Epoch [7130/10000], Loss: 0.6312, Training Accuracy: 0.6215\n",
            "Epoch [7140/10000], Loss: 0.6285, Training Accuracy: 0.6240\n",
            "Epoch [7150/10000], Loss: 0.6289, Training Accuracy: 0.6262\n",
            "Epoch [7160/10000], Loss: 0.6284, Training Accuracy: 0.6218\n",
            "Epoch [7170/10000], Loss: 0.6293, Training Accuracy: 0.6246\n",
            "Epoch [7180/10000], Loss: 0.6288, Training Accuracy: 0.6230\n",
            "Epoch [7190/10000], Loss: 0.6285, Training Accuracy: 0.6256\n",
            "Epoch [7200/10000], Loss: 0.6298, Training Accuracy: 0.6218\n",
            "Epoch [7210/10000], Loss: 0.6282, Training Accuracy: 0.6271\n",
            "Epoch [7220/10000], Loss: 0.6290, Training Accuracy: 0.6271\n",
            "Epoch [7230/10000], Loss: 0.6296, Training Accuracy: 0.6271\n",
            "Epoch [7240/10000], Loss: 0.6293, Training Accuracy: 0.6252\n",
            "Epoch [7250/10000], Loss: 0.6281, Training Accuracy: 0.6281\n",
            "Epoch [7260/10000], Loss: 0.6288, Training Accuracy: 0.6281\n",
            "Epoch [7270/10000], Loss: 0.6288, Training Accuracy: 0.6268\n",
            "Epoch [7280/10000], Loss: 0.6281, Training Accuracy: 0.6274\n",
            "Epoch [7290/10000], Loss: 0.6285, Training Accuracy: 0.6268\n",
            "Epoch [7300/10000], Loss: 0.6319, Training Accuracy: 0.6167\n",
            "Epoch [7310/10000], Loss: 0.6296, Training Accuracy: 0.6249\n",
            "Epoch [7320/10000], Loss: 0.6284, Training Accuracy: 0.6262\n",
            "Epoch [7330/10000], Loss: 0.6280, Training Accuracy: 0.6268\n",
            "Epoch [7340/10000], Loss: 0.6282, Training Accuracy: 0.6237\n",
            "Epoch [7350/10000], Loss: 0.6285, Training Accuracy: 0.6252\n",
            "Epoch [7360/10000], Loss: 0.6287, Training Accuracy: 0.6259\n",
            "Epoch [7370/10000], Loss: 0.6288, Training Accuracy: 0.6262\n",
            "Epoch [7380/10000], Loss: 0.6283, Training Accuracy: 0.6243\n",
            "Epoch [7390/10000], Loss: 0.6291, Training Accuracy: 0.6265\n",
            "Epoch [7400/10000], Loss: 0.6286, Training Accuracy: 0.6237\n",
            "Epoch [7410/10000], Loss: 0.6279, Training Accuracy: 0.6243\n",
            "Epoch [7420/10000], Loss: 0.6288, Training Accuracy: 0.6271\n",
            "Epoch [7430/10000], Loss: 0.6290, Training Accuracy: 0.6243\n",
            "Epoch [7440/10000], Loss: 0.6285, Training Accuracy: 0.6271\n",
            "Epoch [7450/10000], Loss: 0.6283, Training Accuracy: 0.6249\n",
            "Epoch [7460/10000], Loss: 0.6278, Training Accuracy: 0.6224\n",
            "Epoch [7470/10000], Loss: 0.6290, Training Accuracy: 0.6218\n",
            "Epoch [7480/10000], Loss: 0.6288, Training Accuracy: 0.6271\n",
            "Epoch [7490/10000], Loss: 0.6288, Training Accuracy: 0.6268\n",
            "Epoch [7500/10000], Loss: 0.6278, Training Accuracy: 0.6262\n",
            "Epoch [7510/10000], Loss: 0.6278, Training Accuracy: 0.6265\n",
            "Epoch [7520/10000], Loss: 0.6294, Training Accuracy: 0.6202\n",
            "Epoch [7530/10000], Loss: 0.6278, Training Accuracy: 0.6240\n",
            "Epoch [7540/10000], Loss: 0.6286, Training Accuracy: 0.6274\n",
            "Epoch [7550/10000], Loss: 0.6283, Training Accuracy: 0.6256\n",
            "Epoch [7560/10000], Loss: 0.6276, Training Accuracy: 0.6237\n",
            "Epoch [7570/10000], Loss: 0.6286, Training Accuracy: 0.6271\n",
            "Epoch [7580/10000], Loss: 0.6290, Training Accuracy: 0.6256\n",
            "Epoch [7590/10000], Loss: 0.6289, Training Accuracy: 0.6265\n",
            "Epoch [7600/10000], Loss: 0.6276, Training Accuracy: 0.6265\n",
            "Epoch [7610/10000], Loss: 0.6280, Training Accuracy: 0.6274\n",
            "Epoch [7620/10000], Loss: 0.6288, Training Accuracy: 0.6265\n",
            "Epoch [7630/10000], Loss: 0.6276, Training Accuracy: 0.6243\n",
            "Epoch [7640/10000], Loss: 0.6277, Training Accuracy: 0.6246\n",
            "Epoch [7650/10000], Loss: 0.6300, Training Accuracy: 0.6215\n",
            "Epoch [7660/10000], Loss: 0.6275, Training Accuracy: 0.6233\n",
            "Epoch [7670/10000], Loss: 0.6280, Training Accuracy: 0.6271\n",
            "Epoch [7680/10000], Loss: 0.6277, Training Accuracy: 0.6252\n",
            "Epoch [7690/10000], Loss: 0.6280, Training Accuracy: 0.6265\n",
            "Epoch [7700/10000], Loss: 0.6286, Training Accuracy: 0.6281\n",
            "Epoch [7710/10000], Loss: 0.6280, Training Accuracy: 0.6262\n",
            "Epoch [7720/10000], Loss: 0.6274, Training Accuracy: 0.6252\n",
            "Epoch [7730/10000], Loss: 0.6277, Training Accuracy: 0.6284\n",
            "Epoch [7740/10000], Loss: 0.6311, Training Accuracy: 0.6177\n",
            "Epoch [7750/10000], Loss: 0.6280, Training Accuracy: 0.6290\n",
            "Epoch [7760/10000], Loss: 0.6273, Training Accuracy: 0.6233\n",
            "Epoch [7770/10000], Loss: 0.6277, Training Accuracy: 0.6268\n",
            "Epoch [7780/10000], Loss: 0.6274, Training Accuracy: 0.6224\n",
            "Epoch [7790/10000], Loss: 0.6276, Training Accuracy: 0.6274\n",
            "Epoch [7800/10000], Loss: 0.6279, Training Accuracy: 0.6290\n",
            "Epoch [7810/10000], Loss: 0.6295, Training Accuracy: 0.6262\n",
            "Epoch [7820/10000], Loss: 0.6273, Training Accuracy: 0.6256\n",
            "Epoch [7830/10000], Loss: 0.6281, Training Accuracy: 0.6268\n",
            "Epoch [7840/10000], Loss: 0.6275, Training Accuracy: 0.6265\n",
            "Epoch [7850/10000], Loss: 0.6272, Training Accuracy: 0.6268\n",
            "Epoch [7860/10000], Loss: 0.6272, Training Accuracy: 0.6249\n",
            "Epoch [7870/10000], Loss: 0.6304, Training Accuracy: 0.6192\n",
            "Epoch [7880/10000], Loss: 0.6276, Training Accuracy: 0.6271\n",
            "Epoch [7890/10000], Loss: 0.6281, Training Accuracy: 0.6303\n",
            "Epoch [7900/10000], Loss: 0.6278, Training Accuracy: 0.6271\n",
            "Epoch [7910/10000], Loss: 0.6275, Training Accuracy: 0.6284\n",
            "Epoch [7920/10000], Loss: 0.6273, Training Accuracy: 0.6265\n",
            "Epoch [7930/10000], Loss: 0.6271, Training Accuracy: 0.6227\n",
            "Epoch [7940/10000], Loss: 0.6271, Training Accuracy: 0.6249\n",
            "Epoch [7950/10000], Loss: 0.6271, Training Accuracy: 0.6230\n",
            "Epoch [7960/10000], Loss: 0.6276, Training Accuracy: 0.6281\n",
            "Epoch [7970/10000], Loss: 0.6302, Training Accuracy: 0.6240\n",
            "Epoch [7980/10000], Loss: 0.6281, Training Accuracy: 0.6246\n",
            "Epoch [7990/10000], Loss: 0.6270, Training Accuracy: 0.6224\n",
            "Epoch [8000/10000], Loss: 0.6274, Training Accuracy: 0.6278\n",
            "Epoch [8010/10000], Loss: 0.6270, Training Accuracy: 0.6224\n",
            "Epoch [8020/10000], Loss: 0.6274, Training Accuracy: 0.6271\n",
            "Epoch [8030/10000], Loss: 0.6293, Training Accuracy: 0.6192\n",
            "Epoch [8040/10000], Loss: 0.6269, Training Accuracy: 0.6243\n",
            "Epoch [8050/10000], Loss: 0.6277, Training Accuracy: 0.6290\n",
            "Epoch [8060/10000], Loss: 0.6270, Training Accuracy: 0.6237\n",
            "Epoch [8070/10000], Loss: 0.6270, Training Accuracy: 0.6271\n",
            "Epoch [8080/10000], Loss: 0.6290, Training Accuracy: 0.6189\n",
            "Epoch [8090/10000], Loss: 0.6271, Training Accuracy: 0.6256\n",
            "Epoch [8100/10000], Loss: 0.6272, Training Accuracy: 0.6240\n",
            "Epoch [8110/10000], Loss: 0.6272, Training Accuracy: 0.6249\n",
            "Epoch [8120/10000], Loss: 0.6271, Training Accuracy: 0.6259\n",
            "Epoch [8130/10000], Loss: 0.6269, Training Accuracy: 0.6237\n",
            "Epoch [8140/10000], Loss: 0.6268, Training Accuracy: 0.6233\n",
            "Epoch [8150/10000], Loss: 0.6269, Training Accuracy: 0.6249\n",
            "Epoch [8160/10000], Loss: 0.6310, Training Accuracy: 0.6237\n",
            "Epoch [8170/10000], Loss: 0.6274, Training Accuracy: 0.6268\n",
            "Epoch [8180/10000], Loss: 0.6270, Training Accuracy: 0.6290\n",
            "Epoch [8190/10000], Loss: 0.6268, Training Accuracy: 0.6259\n",
            "Epoch [8200/10000], Loss: 0.6267, Training Accuracy: 0.6240\n",
            "Epoch [8210/10000], Loss: 0.6269, Training Accuracy: 0.6243\n",
            "Epoch [8220/10000], Loss: 0.6267, Training Accuracy: 0.6249\n",
            "Epoch [8230/10000], Loss: 0.6267, Training Accuracy: 0.6227\n",
            "Epoch [8240/10000], Loss: 0.6278, Training Accuracy: 0.6281\n",
            "Epoch [8250/10000], Loss: 0.6287, Training Accuracy: 0.6274\n",
            "Epoch [8260/10000], Loss: 0.6282, Training Accuracy: 0.6278\n",
            "Epoch [8270/10000], Loss: 0.6271, Training Accuracy: 0.6293\n",
            "Epoch [8280/10000], Loss: 0.6266, Training Accuracy: 0.6221\n",
            "Epoch [8290/10000], Loss: 0.6269, Training Accuracy: 0.6281\n",
            "Epoch [8300/10000], Loss: 0.6271, Training Accuracy: 0.6290\n",
            "Epoch [8310/10000], Loss: 0.6273, Training Accuracy: 0.6300\n",
            "Epoch [8320/10000], Loss: 0.6275, Training Accuracy: 0.6278\n",
            "Epoch [8330/10000], Loss: 0.6273, Training Accuracy: 0.6290\n",
            "Epoch [8340/10000], Loss: 0.6266, Training Accuracy: 0.6249\n",
            "Epoch [8350/10000], Loss: 0.6266, Training Accuracy: 0.6243\n",
            "Epoch [8360/10000], Loss: 0.6293, Training Accuracy: 0.6177\n",
            "Epoch [8370/10000], Loss: 0.6265, Training Accuracy: 0.6274\n",
            "Epoch [8380/10000], Loss: 0.6266, Training Accuracy: 0.6281\n",
            "Epoch [8390/10000], Loss: 0.6265, Training Accuracy: 0.6268\n",
            "Epoch [8400/10000], Loss: 0.6265, Training Accuracy: 0.6224\n",
            "Epoch [8410/10000], Loss: 0.6266, Training Accuracy: 0.6268\n",
            "Epoch [8420/10000], Loss: 0.6264, Training Accuracy: 0.6249\n",
            "Epoch [8430/10000], Loss: 0.6265, Training Accuracy: 0.6246\n",
            "Epoch [8440/10000], Loss: 0.6275, Training Accuracy: 0.6265\n",
            "Epoch [8450/10000], Loss: 0.6280, Training Accuracy: 0.6290\n",
            "Epoch [8460/10000], Loss: 0.6271, Training Accuracy: 0.6315\n",
            "Epoch [8470/10000], Loss: 0.6264, Training Accuracy: 0.6252\n",
            "Epoch [8480/10000], Loss: 0.6264, Training Accuracy: 0.6237\n",
            "Epoch [8490/10000], Loss: 0.6268, Training Accuracy: 0.6303\n",
            "Epoch [8500/10000], Loss: 0.6308, Training Accuracy: 0.6202\n",
            "Epoch [8510/10000], Loss: 0.6274, Training Accuracy: 0.6224\n",
            "Epoch [8520/10000], Loss: 0.6265, Training Accuracy: 0.6300\n",
            "Epoch [8530/10000], Loss: 0.6263, Training Accuracy: 0.6237\n",
            "Epoch [8540/10000], Loss: 0.6265, Training Accuracy: 0.6287\n",
            "Epoch [8550/10000], Loss: 0.6263, Training Accuracy: 0.6259\n",
            "Epoch [8560/10000], Loss: 0.6263, Training Accuracy: 0.6233\n",
            "Epoch [8570/10000], Loss: 0.6278, Training Accuracy: 0.6189\n",
            "Epoch [8580/10000], Loss: 0.6273, Training Accuracy: 0.6274\n",
            "Epoch [8590/10000], Loss: 0.6271, Training Accuracy: 0.6274\n",
            "Epoch [8600/10000], Loss: 0.6267, Training Accuracy: 0.6287\n",
            "Epoch [8610/10000], Loss: 0.6266, Training Accuracy: 0.6271\n",
            "Epoch [8620/10000], Loss: 0.6262, Training Accuracy: 0.6243\n",
            "Epoch [8630/10000], Loss: 0.6266, Training Accuracy: 0.6278\n",
            "Epoch [8640/10000], Loss: 0.6267, Training Accuracy: 0.6278\n",
            "Epoch [8650/10000], Loss: 0.6270, Training Accuracy: 0.6297\n",
            "Epoch [8660/10000], Loss: 0.6272, Training Accuracy: 0.6293\n",
            "Epoch [8670/10000], Loss: 0.6264, Training Accuracy: 0.6287\n",
            "Epoch [8680/10000], Loss: 0.6261, Training Accuracy: 0.6224\n",
            "Epoch [8690/10000], Loss: 0.6261, Training Accuracy: 0.6259\n",
            "Epoch [8700/10000], Loss: 0.6263, Training Accuracy: 0.6284\n",
            "Epoch [8710/10000], Loss: 0.6369, Training Accuracy: 0.6117\n",
            "Epoch [8720/10000], Loss: 0.6284, Training Accuracy: 0.6303\n",
            "Epoch [8730/10000], Loss: 0.6266, Training Accuracy: 0.6293\n",
            "Epoch [8740/10000], Loss: 0.6265, Training Accuracy: 0.6287\n",
            "Epoch [8750/10000], Loss: 0.6261, Training Accuracy: 0.6237\n",
            "Epoch [8760/10000], Loss: 0.6260, Training Accuracy: 0.6230\n",
            "Epoch [8770/10000], Loss: 0.6260, Training Accuracy: 0.6246\n",
            "Epoch [8780/10000], Loss: 0.6260, Training Accuracy: 0.6221\n",
            "Epoch [8790/10000], Loss: 0.6260, Training Accuracy: 0.6221\n",
            "Epoch [8800/10000], Loss: 0.6260, Training Accuracy: 0.6233\n",
            "Epoch [8810/10000], Loss: 0.6261, Training Accuracy: 0.6274\n",
            "Epoch [8820/10000], Loss: 0.6271, Training Accuracy: 0.6278\n",
            "Epoch [8830/10000], Loss: 0.6260, Training Accuracy: 0.6249\n",
            "Epoch [8840/10000], Loss: 0.6264, Training Accuracy: 0.6300\n",
            "Epoch [8850/10000], Loss: 0.6261, Training Accuracy: 0.6300\n",
            "Epoch [8860/10000], Loss: 0.6260, Training Accuracy: 0.6297\n",
            "Epoch [8870/10000], Loss: 0.6276, Training Accuracy: 0.6215\n",
            "Epoch [8880/10000], Loss: 0.6265, Training Accuracy: 0.6300\n",
            "Epoch [8890/10000], Loss: 0.6269, Training Accuracy: 0.6312\n",
            "Epoch [8900/10000], Loss: 0.6260, Training Accuracy: 0.6300\n",
            "Epoch [8910/10000], Loss: 0.6260, Training Accuracy: 0.6297\n",
            "Epoch [8920/10000], Loss: 0.6259, Training Accuracy: 0.6249\n",
            "Epoch [8930/10000], Loss: 0.6262, Training Accuracy: 0.6306\n",
            "Epoch [8940/10000], Loss: 0.6292, Training Accuracy: 0.6186\n",
            "Epoch [8950/10000], Loss: 0.6260, Training Accuracy: 0.6306\n",
            "Epoch [8960/10000], Loss: 0.6259, Training Accuracy: 0.6237\n",
            "Epoch [8970/10000], Loss: 0.6262, Training Accuracy: 0.6290\n",
            "Epoch [8980/10000], Loss: 0.6258, Training Accuracy: 0.6233\n",
            "Epoch [8990/10000], Loss: 0.6258, Training Accuracy: 0.6259\n",
            "Epoch [9000/10000], Loss: 0.6276, Training Accuracy: 0.6278\n",
            "Epoch [9010/10000], Loss: 0.6264, Training Accuracy: 0.6249\n",
            "Epoch [9020/10000], Loss: 0.6269, Training Accuracy: 0.6300\n",
            "Epoch [9030/10000], Loss: 0.6261, Training Accuracy: 0.6303\n",
            "Epoch [9040/10000], Loss: 0.6257, Training Accuracy: 0.6237\n",
            "Epoch [9050/10000], Loss: 0.6260, Training Accuracy: 0.6284\n",
            "Epoch [9060/10000], Loss: 0.6263, Training Accuracy: 0.6300\n",
            "Epoch [9070/10000], Loss: 0.6266, Training Accuracy: 0.6315\n",
            "Epoch [9080/10000], Loss: 0.6260, Training Accuracy: 0.6297\n",
            "Epoch [9090/10000], Loss: 0.6257, Training Accuracy: 0.6224\n",
            "Epoch [9100/10000], Loss: 0.6261, Training Accuracy: 0.6303\n",
            "Epoch [9110/10000], Loss: 0.6309, Training Accuracy: 0.6174\n",
            "Epoch [9120/10000], Loss: 0.6274, Training Accuracy: 0.6287\n",
            "Epoch [9130/10000], Loss: 0.6262, Training Accuracy: 0.6297\n",
            "Epoch [9140/10000], Loss: 0.6257, Training Accuracy: 0.6262\n",
            "Epoch [9150/10000], Loss: 0.6255, Training Accuracy: 0.6249\n",
            "Epoch [9160/10000], Loss: 0.6257, Training Accuracy: 0.6303\n",
            "Epoch [9170/10000], Loss: 0.6257, Training Accuracy: 0.6290\n",
            "Epoch [9180/10000], Loss: 0.6259, Training Accuracy: 0.6303\n",
            "Epoch [9190/10000], Loss: 0.6281, Training Accuracy: 0.6192\n",
            "Epoch [9200/10000], Loss: 0.6255, Training Accuracy: 0.6243\n",
            "Epoch [9210/10000], Loss: 0.6262, Training Accuracy: 0.6325\n",
            "Epoch [9220/10000], Loss: 0.6255, Training Accuracy: 0.6246\n",
            "Epoch [9230/10000], Loss: 0.6259, Training Accuracy: 0.6300\n",
            "Epoch [9240/10000], Loss: 0.6274, Training Accuracy: 0.6202\n",
            "Epoch [9250/10000], Loss: 0.6256, Training Accuracy: 0.6240\n",
            "Epoch [9260/10000], Loss: 0.6257, Training Accuracy: 0.6297\n",
            "Epoch [9270/10000], Loss: 0.6261, Training Accuracy: 0.6344\n",
            "Epoch [9280/10000], Loss: 0.6264, Training Accuracy: 0.6322\n",
            "Epoch [9290/10000], Loss: 0.6256, Training Accuracy: 0.6274\n",
            "Epoch [9300/10000], Loss: 0.6257, Training Accuracy: 0.6268\n",
            "Epoch [9310/10000], Loss: 0.6271, Training Accuracy: 0.6287\n",
            "Epoch [9320/10000], Loss: 0.6259, Training Accuracy: 0.6249\n",
            "Epoch [9330/10000], Loss: 0.6263, Training Accuracy: 0.6303\n",
            "Epoch [9340/10000], Loss: 0.6253, Training Accuracy: 0.6237\n",
            "Epoch [9350/10000], Loss: 0.6257, Training Accuracy: 0.6284\n",
            "Epoch [9360/10000], Loss: 0.6262, Training Accuracy: 0.6328\n",
            "Epoch [9370/10000], Loss: 0.6258, Training Accuracy: 0.6325\n",
            "Epoch [9380/10000], Loss: 0.6258, Training Accuracy: 0.6334\n",
            "Epoch [9390/10000], Loss: 0.6267, Training Accuracy: 0.6293\n",
            "Epoch [9400/10000], Loss: 0.6256, Training Accuracy: 0.6268\n",
            "Epoch [9410/10000], Loss: 0.6254, Training Accuracy: 0.6331\n",
            "Epoch [9420/10000], Loss: 0.6271, Training Accuracy: 0.6211\n",
            "Epoch [9430/10000], Loss: 0.6251, Training Accuracy: 0.6259\n",
            "Epoch [9440/10000], Loss: 0.6256, Training Accuracy: 0.6319\n",
            "Epoch [9450/10000], Loss: 0.6252, Training Accuracy: 0.6306\n",
            "Epoch [9460/10000], Loss: 0.6247, Training Accuracy: 0.6259\n",
            "Epoch [9470/10000], Loss: 0.6249, Training Accuracy: 0.6312\n",
            "Epoch [9480/10000], Loss: 0.6288, Training Accuracy: 0.6174\n",
            "Epoch [9490/10000], Loss: 0.6252, Training Accuracy: 0.6290\n",
            "Epoch [9500/10000], Loss: 0.6248, Training Accuracy: 0.6347\n",
            "Epoch [9510/10000], Loss: 0.6247, Training Accuracy: 0.6297\n",
            "Epoch [9520/10000], Loss: 0.6245, Training Accuracy: 0.6290\n",
            "Epoch [9530/10000], Loss: 0.6245, Training Accuracy: 0.6243\n",
            "Epoch [9540/10000], Loss: 0.6245, Training Accuracy: 0.6221\n",
            "Epoch [9550/10000], Loss: 0.6245, Training Accuracy: 0.6243\n",
            "Epoch [9560/10000], Loss: 0.6255, Training Accuracy: 0.6293\n",
            "Epoch [9570/10000], Loss: 0.6260, Training Accuracy: 0.6300\n",
            "Epoch [9580/10000], Loss: 0.6246, Training Accuracy: 0.6344\n",
            "Epoch [9590/10000], Loss: 0.6248, Training Accuracy: 0.6350\n",
            "Epoch [9600/10000], Loss: 0.6243, Training Accuracy: 0.6259\n",
            "Epoch [9610/10000], Loss: 0.6249, Training Accuracy: 0.6353\n",
            "Epoch [9620/10000], Loss: 0.6274, Training Accuracy: 0.6281\n",
            "Epoch [9630/10000], Loss: 0.6251, Training Accuracy: 0.6322\n",
            "Epoch [9640/10000], Loss: 0.6243, Training Accuracy: 0.6240\n",
            "Epoch [9650/10000], Loss: 0.6247, Training Accuracy: 0.6281\n",
            "Epoch [9660/10000], Loss: 0.6242, Training Accuracy: 0.6243\n",
            "Epoch [9670/10000], Loss: 0.6242, Training Accuracy: 0.6256\n",
            "Epoch [9680/10000], Loss: 0.6249, Training Accuracy: 0.6319\n",
            "Epoch [9690/10000], Loss: 0.6282, Training Accuracy: 0.6224\n",
            "Epoch [9700/10000], Loss: 0.6260, Training Accuracy: 0.6300\n",
            "Epoch [9710/10000], Loss: 0.6249, Training Accuracy: 0.6347\n",
            "Epoch [9720/10000], Loss: 0.6245, Training Accuracy: 0.6293\n",
            "Epoch [9730/10000], Loss: 0.6242, Training Accuracy: 0.6297\n",
            "Epoch [9740/10000], Loss: 0.6243, Training Accuracy: 0.6284\n",
            "Epoch [9750/10000], Loss: 0.6242, Training Accuracy: 0.6300\n",
            "Epoch [9760/10000], Loss: 0.6244, Training Accuracy: 0.6344\n",
            "Epoch [9770/10000], Loss: 0.6264, Training Accuracy: 0.6221\n",
            "Epoch [9780/10000], Loss: 0.6241, Training Accuracy: 0.6252\n",
            "Epoch [9790/10000], Loss: 0.6248, Training Accuracy: 0.6338\n",
            "Epoch [9800/10000], Loss: 0.6241, Training Accuracy: 0.6218\n",
            "Epoch [9810/10000], Loss: 0.6240, Training Accuracy: 0.6246\n",
            "Epoch [9820/10000], Loss: 0.6246, Training Accuracy: 0.6328\n",
            "Epoch [9830/10000], Loss: 0.6291, Training Accuracy: 0.6202\n",
            "Epoch [9840/10000], Loss: 0.6262, Training Accuracy: 0.6293\n",
            "Epoch [9850/10000], Loss: 0.6248, Training Accuracy: 0.6341\n",
            "Epoch [9860/10000], Loss: 0.6243, Training Accuracy: 0.6278\n",
            "Epoch [9870/10000], Loss: 0.6240, Training Accuracy: 0.6306\n",
            "Epoch [9880/10000], Loss: 0.6240, Training Accuracy: 0.6287\n",
            "Epoch [9890/10000], Loss: 0.6240, Training Accuracy: 0.6259\n",
            "Epoch [9900/10000], Loss: 0.6242, Training Accuracy: 0.6344\n",
            "Epoch [9910/10000], Loss: 0.6266, Training Accuracy: 0.6208\n",
            "Epoch [9920/10000], Loss: 0.6239, Training Accuracy: 0.6268\n",
            "Epoch [9930/10000], Loss: 0.6246, Training Accuracy: 0.6338\n",
            "Epoch [9940/10000], Loss: 0.6239, Training Accuracy: 0.6274\n",
            "Epoch [9950/10000], Loss: 0.6246, Training Accuracy: 0.6344\n",
            "Epoch [9960/10000], Loss: 0.6246, Training Accuracy: 0.6350\n",
            "Epoch [9970/10000], Loss: 0.6238, Training Accuracy: 0.6237\n",
            "Epoch [9980/10000], Loss: 0.6245, Training Accuracy: 0.6356\n",
            "Epoch [9990/10000], Loss: 0.6267, Training Accuracy: 0.6274\n",
            "Epoch [10000/10000], Loss: 0.6244, Training Accuracy: 0.6338\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, input_tensor, true_labels):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct = (predicted == true_labels).float().sum()\n",
        "        accuracy = correct / len(true_labels)\n",
        "        return accuracy.item()\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_features, dtype=torch.float32).to(device)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "print(X_train_tensor.shape, Y_train_tensor.shape)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(denseModel.parameters(), lr=0.001)\n",
        "\n",
        "bestValAcc = 0\n",
        "# Training loop\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    denseModel.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = denseModel(X_train_tensor)\n",
        "    loss = criterion(outputs, Y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute training accuracy\n",
        "    train_accuracy = compute_accuracy(denseModel, X_train_tensor, Y_train_tensor)\n",
        "\n",
        "    # Uncomment this if you want to see how the accuracy of testing improves during the training process.\n",
        "    ##Compute testing accuracy\n",
        "    # X_val_tensor = torch.tensor(X_val_features, dtype=torch.float32).to(device)\n",
        "    # Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    # val_accuracy = compute_accuracy(denseModel, X_val_tensor, Y_val_tensor)\n",
        "\n",
        "    # if bestValAcc < val_accuracy:\n",
        "    #     bestValAcc = val_accuracy\n",
        "    #     print(f'Saving model with best validation accuracy ...')\n",
        "    #     torch.save(denseModel.state_dict(), 'llama-' + task + '-best-model')\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.4f}\"\n",
        "        )\n",
        "        # \"Validation Accuracy\": {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE9X728RRHRK"
      },
      "source": [
        "#Compute the metrics using the model on the Test Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ntxC26m--ma",
        "outputId": "8c3a3435-7314-42cb-984b-f110120af1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing - Accuracy: 0.5901, Precision: 0.8430, Recall: 0.4883, F1: 0.6184, ROC AUC: 0.7250, PR AUC: 0.8397\n",
            "Testing - Negative: 0.5901, Precision-Negative: 0.4257, Recall-Negative: 0.8066, F1-Negative: 0.5573, ROC AUC-Negative: 0.2750, PR AUC-Negative: 0.5501\n"
          ]
        }
      ],
      "source": [
        "X_test_tensor = torch.tensor(X_test_features, dtype=torch.float32).to(device)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "test_metrics = compute_metrics(denseModel, X_test_tensor, Y_test_tensor)\n",
        "\n",
        "print(\n",
        "    f\"Testing - Accuracy: {test_metrics['Accuracy']:.4f}, Precision: {test_metrics['Precision']:.4f}, Recall: {test_metrics['Recall']:.4f}, F1: {test_metrics['F1']:.4f}, ROC AUC: {test_metrics['ROC AUC']:.4f}, PR AUC: {test_metrics['PR AUC']:.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"Testing - Negative: {test_metrics['Accuracy']:.4f}, Precision-Negative: {test_metrics['Precision-Negative']:.4f}, Recall-Negative: {test_metrics['Recall-Negative']:.4f}, F1-Negative: {test_metrics['F1-Negative']:.4f}, ROC AUC-Negative: {test_metrics['ROC AUC-Negative']:.4f}, PR AUC-Negative: {test_metrics['PR AUC-Negative']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-RlivuLRN0H"
      },
      "source": [
        "## Save the results on a CSV if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "boqJczJoL4lW"
      },
      "outputs": [],
      "source": [
        "model_dataframe = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"features\",\n",
        "        \"model_name\",\n",
        "        \"feature_to_extract\",\n",
        "        \"method\",\n",
        "        \"accuracy\",\n",
        "        \"precision\",\n",
        "        \"recall\",\n",
        "        \"roc auc\",\n",
        "        \"pr auc\",\n",
        "        \"negative\",\n",
        "        \"precision-negative\",\n",
        "        \"recall-negative\",\n",
        "        \"negative f1\",\n",
        "        \"lr_accuracy\",\n",
        "        \"lr_features_log\",\n",
        "        \"lr_features_no_log\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NJQ56NLzL5Tv"
      },
      "outputs": [],
      "source": [
        "d = {\n",
        "    \"features\": features_to_extract,\n",
        "    \"model_name\": str(model.getName()),\n",
        "    \"feature_to_extract\": feature_to_extract,\n",
        "    \"method\": \"TEST\",\n",
        "    \"accuracy\": test_metrics[\"Accuracy\"],\n",
        "    \"precision\": test_metrics[\"Precision\"],\n",
        "    \"recall\": test_metrics[\"Recall\"],\n",
        "    \"f1\": test_metrics[\"F1\"],\n",
        "    \"pr auc\": test_metrics[\"PR AUC\"],\n",
        "    \"precision-negative\": test_metrics[\"Precision-Negative\"],\n",
        "    \"recall-negative\": test_metrics[\"Recall-Negative\"],\n",
        "    \"negative-f1\": test_metrics[\"F1-Negative\"],\n",
        "    \"lr_accuracy\": lr_accuracy,\n",
        "    \"lr_features_log\": lr_features_log,\n",
        "    \"lr_features_no_log\": lr_features_no_log,\n",
        "}\n",
        "\n",
        "model_dataframe.loc[len(model_dataframe.index)] = d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XNMNRvxvMO_o"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>model_name</th>\n",
              "      <th>model</th>\n",
              "      <th>method</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>roc auc</th>\n",
              "      <th>pr auc</th>\n",
              "      <th>negative</th>\n",
              "      <th>precision-negative</th>\n",
              "      <th>recall-negative</th>\n",
              "      <th>negative f1</th>\n",
              "      <th>lr_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'mtp': True, 'avgtp': True, 'MDVTP': True, 'M...</td>\n",
              "      <td>EleutherAI/gpt-j-6B</td>\n",
              "      <td>&lt;__main__.Gptj object at 0x1553c2f1c430&gt;</td>\n",
              "      <td>TEST</td>\n",
              "      <td>0.590106</td>\n",
              "      <td>0.843049</td>\n",
              "      <td>0.488312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.839709</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.425656</td>\n",
              "      <td>0.80663</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.662544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            features           model_name  \\\n",
              "0  {'mtp': True, 'avgtp': True, 'MDVTP': True, 'M...  EleutherAI/gpt-j-6B   \n",
              "\n",
              "                                      model method  accuracy  precision  \\\n",
              "0  <__main__.Gptj object at 0x1553c2f1c430>   TEST  0.590106   0.843049   \n",
              "\n",
              "     recall  roc auc    pr auc  negative  precision-negative  recall-negative  \\\n",
              "0  0.488312      NaN  0.839709       NaN            0.425656          0.80663   \n",
              "\n",
              "   negative f1  lr_accuracy  \n",
              "0          NaN     0.662544  "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6tGhxo6SMRvc"
      },
      "outputs": [],
      "source": [
        "csv_name = f\"{model.getSanitizedName()}_mind_test={test_dataset[0]}_{includeConditioned=}_{'_'.join([f'{k}={v}' for k, v in features_to_extract.items()])}.csv\"\n",
        "model_dataframe.to_csv(output_path / csv_name, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
