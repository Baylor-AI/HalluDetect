{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx32UcwtcrK5"
      },
      "source": [
        "## Connect to your Google Drive or any filesystem you are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzIyxG0kMvLg",
        "outputId": "2ccabc0d-67fa-477a-8c27-737ba966fd18"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/gdrive\")\n",
        "base_path = (\n",
        "    Path(\"your_path\") if IN_COLAB else Path(\".\")\n",
        ")\n",
        "\n",
        "#Fix if you want to use a different path\n",
        "output_path = base_path / \"output\"\n",
        "data_path = base_path / \"helm\"\n",
        "output_path.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs_MYWyElf9"
      },
      "source": [
        "## These are the libraries needed to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_IRq_2iM8Oh"
      },
      "source": [
        "## Deep Learning Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3JO8-2WM0Oi",
        "outputId": "3c54bbe0-dcbb-4e0e-b3bc-a909df86084c"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets evaluate rouge_score\n",
        "%pip install --upgrade huggingface_hub\n",
        "%pip install accelerate -U\n",
        "%pip install transformers[torch]\n",
        "%pip install sentencepiece\n",
        "%pip install google\n",
        "%pip install protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4I9BbUWNGAT"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_orD5O6M0Sz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulEMNzTdNK6p"
      },
      "source": [
        "# Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBBHKHm1PLgN"
      },
      "outputs": [],
      "source": [
        "datasetNames = ['llamachat7b', 'falcon40b','mpt7b','llamachat13b', 'llamabase7b', 'opt7b', 'gptj7b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterate through each data.json corresponding to each LLM and load the information a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z559eoe9EsIw"
      },
      "outputs": [],
      "source": [
        "def loadDataset(datasetNames):\n",
        "    sentences_list = []\n",
        "    labels_list = []\n",
        "    prompt_list = []\n",
        "\n",
        "    for datasetName in datasetNames:\n",
        "        path = data_path / f\"./data/{datasetName}\"\n",
        "\n",
        "        with open(path / \"data.json\", \"r\") as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "            # Iterate through each entry in the JSON\n",
        "            for key, value in data.items():\n",
        "                prompt = value[\"prompt\"]\n",
        "                for sentence_data in value[\"sentences\"]:\n",
        "                    sentence = sentence_data[\"sentence\"]\n",
        "                    label = sentence_data[\"label\"]\n",
        "\n",
        "                    # Append data to lists\n",
        "                    sentences_list.append(sentence)\n",
        "                    labels_list.append(label)\n",
        "                    prompt_list.append(prompt)\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        {\"Prompt\": prompt_list, \"Sentence\": sentences_list, \"Label\": labels_list}\n",
        "    )\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select the `test_idx` corresponding to the LLM generated data that you want to use for testing and the rest will be taken for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwC9pOCKM0XJ"
      },
      "outputs": [],
      "source": [
        "test_idx = 0\n",
        "train_dataset_names = datasetNames[:test_idx] + datasetNames[test_idx + 1 :]\n",
        "test_dataset = datasetNames[test_idx : test_idx + 1]\n",
        "assert test_dataset[0] not in train_dataset_names\n",
        "train_data = loadDataset(train_dataset_names)\n",
        "test_data = loadDataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'{train_dataset_names=}')\n",
        "print(f'{test_dataset=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jx0urZkZRa9U",
        "outputId": "49d6fd3c-cb94-4961-e110-2f01331273ec"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPhslb8B2LK_",
        "outputId": "86b3800a-cb9e-4963-d5b7-6322bf03fe00"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6PgoaCNRCz"
      },
      "source": [
        "# Setting Device to use the GPU\n",
        "\n",
        "We use the T4 GPU in Colab since the heaviest computation for us is the inference of the LLM-Evaluator. Therefore, T4 seem as the better fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpd-VUHOM0bn",
        "outputId": "1707657f-9640-4afc-ebd1-b4932a72d024"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-imxNzmM0du"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5GcfQIoNWfm"
      },
      "source": [
        "## Generic LLMModel class to reuse the functionality of extracting the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_H0RABQM0f7"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import BartForConditionalGeneration, PegasusForConditionalGeneration\n",
        "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LLMModel:\n",
        "    def __init__(self):\n",
        "        self.model = self.model.to(device)\n",
        "        pass\n",
        "\n",
        "    def getName(self) -> str:\n",
        "        return self.model_name\n",
        "\n",
        "    def getSanitizedName(self) -> str:\n",
        "        return self.model_name.replace(\"/\", \"__\")\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        pass\n",
        "\n",
        "    ##Move in future commits this method to an utils.py\n",
        "    def truncate_string_by_len(self, s, truncate_len):\n",
        "        words = s.split()\n",
        "        truncated_words = words[:-truncate_len] if truncate_len > 0 else words\n",
        "        return \" \".join(truncated_words)\n",
        "\n",
        "    # Method to get the vocabulary probabilities of the LLM for a given token on the generated text from LLM-Generator\n",
        "    def getVocabProbsAtPos(self, pos, token_probs):\n",
        "        sorted_probs, sorted_indices = torch.sort(token_probs[pos, :], descending=True)\n",
        "        return sorted_probs\n",
        "\n",
        "    def getMaxLength(self):\n",
        "        return self.model.config.max_position_embeddings\n",
        "\n",
        "    # By default knowledge is the empty string. If you want to add extra knowledge you can do it and it will work as well.\n",
        "    def extractFeatures(\n",
        "        self,\n",
        "        knowledge=\"\",\n",
        "        conditionted_text=\"\",\n",
        "        generated_text=\"\",\n",
        "        features_to_extract={},\n",
        "    ):\n",
        "        self.model.eval()\n",
        "\n",
        "        # Also in the case of the LED model, there is no need to truncate the text in the context of this dataset.\n",
        "        total_len = len(knowledge) + len(conditionted_text) + len(generated_text)\n",
        "        truncate_len = min(total_len - self.tokenizer.model_max_length, 0)\n",
        "\n",
        "        # Truncate knowledge in case is too large\n",
        "        knowledge = self.truncate_string_by_len(knowledge, truncate_len // 2)\n",
        "        # Truncate text_A in case is too large\n",
        "        conditionted_text = self.truncate_string_by_len(\n",
        "            conditionted_text, truncate_len - (truncate_len // 2)\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            [knowledge + conditionted_text + generated_text],\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.getMaxLength(),\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        for key in inputs:\n",
        "            inputs[key] = inputs[key].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        probs = probs.to(device)\n",
        "\n",
        "        tokens_generated_length = len(self.tokenizer.tokenize(generated_text))\n",
        "        start_index = logits.shape[1] - tokens_generated_length\n",
        "        conditional_probs = probs[0, start_index :]\n",
        "\n",
        "        token_ids_generated = inputs[\"input_ids\"][0, start_index :].tolist()\n",
        "        token_probs_generated = [\n",
        "            conditional_probs[i, tid].item()\n",
        "            for i, tid in enumerate(token_ids_generated)\n",
        "        ]\n",
        "\n",
        "        tokens_generated = self.tokenizer.convert_ids_to_tokens(token_ids_generated)\n",
        "\n",
        "        minimum_token_prob = min(token_probs_generated)\n",
        "        average_token_prob = sum(token_probs_generated) / len(token_probs_generated)\n",
        "\n",
        "        maximum_diff_with_vocab = -1\n",
        "        minimum_vocab_extreme_diff = 100000000000\n",
        "\n",
        "        if features_to_extract[\"MDVTP\"] == True or features_to_extract[\"MMDVP\"] == True:\n",
        "            size = len(token_probs_generated)\n",
        "            for pos in range(size):\n",
        "                vocabProbs = self.getVocabProbsAtPos(pos, conditional_probs)\n",
        "                maximum_diff_with_vocab = max(\n",
        "                    [\n",
        "                        maximum_diff_with_vocab,\n",
        "                        self.getDiffVocab(vocabProbs, token_probs_generated[pos]),\n",
        "                    ]\n",
        "                )\n",
        "                minimum_vocab_extreme_diff = min(\n",
        "                    [\n",
        "                        minimum_vocab_extreme_diff,\n",
        "                        self.getDiffMaximumWithMinimum(vocabProbs),\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "        # allFeatures = [minimum_token_prob, average_token_prob, maximum_diff_with_vocab, minimum_vocab_extreme_diff]\n",
        "\n",
        "        allFeatures = {\n",
        "            \"mtp\": minimum_token_prob,\n",
        "            \"avgtp\": average_token_prob,\n",
        "            \"MDVTP\": maximum_diff_with_vocab,\n",
        "            \"MMDVP\": minimum_vocab_extreme_diff,\n",
        "        }\n",
        "\n",
        "        selectedFeatures = {}\n",
        "        for key, feature in features_to_extract.items():\n",
        "            if feature == True:\n",
        "                selectedFeatures[key] = allFeatures[key]\n",
        "\n",
        "        return selectedFeatures\n",
        "\n",
        "    def getDiffVocab(self, vocabProbs, tprob):\n",
        "        return (vocabProbs[0] - tprob).item()\n",
        "\n",
        "    def getDiffMaximumWithMinimum(self, vocabProbs):\n",
        "        return (vocabProbs[0] - vocabProbs[-1]).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTqGHAYvIyEc"
      },
      "source": [
        "## Definition of the specific Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTxfgFM3Iwuh"
      },
      "outputs": [],
      "source": [
        "\"\"\"For now there is code repetition, but it helps to understand the details of each model as separate. However, will make\n",
        "everything with better programming practices by using the AutoModel alternatives of HuggingFace.\"\"\"\n",
        "\n",
        "\n",
        "class Gemma(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"google/gemma-7b-it\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class LLama(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=1024, return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "class Opt(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/opt-6.7b\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        " \n",
        " \n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        " \n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        " \n",
        "        return summary\n",
        "\n",
        "\n",
        "class Gptj(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"EleutherAI/gpt-j-6B\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        " \n",
        " \n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        " \n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        " \n",
        "        return summary\n",
        "\n",
        "\n",
        "class BartCNN(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"facebook/bart-large-cnn\"\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary\n",
        "\n",
        "class GPT2Generator(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gpt2-large\"\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer.encode(\n",
        "            inpt, return_tensors=\"pt\", max_length=self.getMaxLength(), truncation=True\n",
        "        )\n",
        "        output_ids = self.model.generate(\n",
        "            inputs, max_length=1024, num_return_sequences=1\n",
        "        )\n",
        "        output = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LED(LLMModel):\n",
        "    def __init__(self):\n",
        "        self.model_name = \"allenai/led-large-16384-arxiv\"\n",
        "        self.model = LEDForConditionalGeneration.from_pretrained(self.model_name)\n",
        "        self.tokenizer = LEDTokenizer.from_pretrained(self.model_name)\n",
        "        super().__init__()\n",
        "\n",
        "    def generate(self, inpt):\n",
        "        inputs = self.tokenizer(\n",
        "            [inpt], max_length=self.getMaxLength(), return_tensors=\"pt\", truncation=True\n",
        "        )\n",
        "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
        "\n",
        "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtKuMaZNQ8MO"
      },
      "source": [
        "# The Dictionary `features_to_extract` defines which features will be use in this experiment.\n",
        "\n",
        "## Features Meaning:\n",
        "\n",
        "- `mtp` : Take the minimum of the probabilities that the LLM_E gives to the tokens on the generated-text.\n",
        "- `avgtp` : Take the average of the probabilities that the LLM_E\n",
        "gives to the tokens on the generated-text.\n",
        "- `MDVTP` : Take the maximum from all the differences\n",
        "between the token with the highest probability\n",
        "according to LLM_E at position i and the\n",
        "assigned probability from LLM_E to the token at position i in the generated_text.\n",
        "- `MMDVP` : Take the maximum from all the differences between the token with the highest probability according to $LLM_E$ at position $i$ ($v^*$) and the token with the lowest probability according to $LLM_E$ at position $i$ ($v^-$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OjJxW1PRHrE"
      },
      "outputs": [],
      "source": [
        "feature_to_extract = 'all'\n",
        "\n",
        "available_features_to_extract = [\"mtp\", \"avgtp\", \"MDVTP\", \"MMDVP\"]\n",
        "if feature_to_extract == 'all':\n",
        "    features_to_extract = {\n",
        "        feature: True for feature in available_features_to_extract\n",
        "    }\n",
        "else:\n",
        "    features_to_extract = {\n",
        "        feature: True if feature == feature_to_extract else False\n",
        "        for feature in available_features_to_extract\n",
        "    }\n",
        "\n",
        "features_to_extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GdklJnQIB8"
      },
      "source": [
        "## This cell is to instantiate the model you intend to use for the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZHYra4sM0ig"
      },
      "outputs": [],
      "source": [
        "# model = BartCNN()\n",
        "# model = LED()\n",
        "model = GPT2Generator()\n",
        "#model = LLama()\n",
        "# model = Gemma()\n",
        "# model = Opt()\n",
        "# model = Gptj()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpgpfLjKQiwe"
      },
      "source": [
        "##Cleaning Cache on GPU to save memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t5DYYzaM0nE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUXW1LNxQnal"
      },
      "source": [
        "## This cell creates the dataset separation. For this dataset the separation was done by `test_idx` in previous cells. However in here we adapted to the format of our experiments of `(condition-text, generated-text, label [is or not an hallucination])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "includeConditioned = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCPqy3UTLrJa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def adaptDataset(\n",
        "    train_data: pd.DataFrame, test_data: pd.DataFrame, includeConditioned: bool\n",
        "):\n",
        "\n",
        "    dataset_train = []\n",
        "    dataset_test = []\n",
        "    for _, row in train_data.iterrows():\n",
        "        prompt, text, hallu = row[\"Prompt\"], row[\"Sentence\"], row[\"Label\"]\n",
        "        dataset_train.append((prompt, text, hallu))\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        prompt, text, hallu = row[\"Prompt\"], row[\"Sentence\"], row[\"Label\"]\n",
        "        dataset_test.append((prompt, text, hallu))\n",
        "\n",
        "    random.shuffle(dataset_train)\n",
        "    random.shuffle(dataset_test)\n",
        "\n",
        "    X_train = [(p if includeConditioned else \"\", t) for p, t, _ in dataset_train]\n",
        "    Y_train = [y for _, _, y, in dataset_train]\n",
        "\n",
        "    X_test = [(p if includeConditioned else \"\", t) for p, t, _ in dataset_test]\n",
        "    Y_test = [y for _, _, y, in dataset_test]\n",
        "\n",
        "    return X_train, Y_train, [], [], X_test, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBUtDsPsPFy3"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train, X_val, Y_val, X_test, Y_test = adaptDataset(train_data, test_data, includeConditioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mHLWT2nrcli",
        "outputId": "3b8492a2-95d5-42db-829f-45ef5aa64269"
      },
      "outputs": [],
      "source": [
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))\n",
        "print(len(X_test), len(Y_test)) #verify the sizes look right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2BNY5b1M0ri",
        "outputId": "94bbaeee-9be4-4167-d1b2-93c0b5f0aabd"
      },
      "outputs": [],
      "source": [
        "print(len(X_train), len(Y_train))\n",
        "print(len(X_val), len(Y_val))\n",
        "print(len(X_test), len(Y_test)) #verify the sizes look right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDHkOclDQMDW",
        "outputId": "53192fee-05ca-4d31-fb0e-69cf3659a6ec"
      },
      "outputs": [],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo_9Sd3n9BYA",
        "outputId": "96014aed-6618-4d8f-efc4-154311d88c7d"
      },
      "outputs": [],
      "source": [
        "Y_test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNNRZuisSUrf"
      },
      "source": [
        "## Extracting the features for the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtdBbmC8QgoK",
        "outputId": "214d8aef-9bbe-4a50-9ae4-4926ae7d006c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def extract_features(\n",
        "    knowledge: str,\n",
        "    conditioned_text: str,\n",
        "    generated_text: str,\n",
        "    features_to_extract: dict[str, bool],\n",
        "):\n",
        "    return model.extractFeatures(\n",
        "        knowledge, conditioned_text, generated_text, features_to_extract\n",
        "    )\n",
        "\n",
        "X_train_features_maps = []\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_train, desc=\"Processing\"):\n",
        "    X_train_features_maps.append(\n",
        "        extract_features(\n",
        "            \"\", conditioned_text, generated_text, features_to_extract\n",
        "        )\n",
        "    )\n",
        "    torch.cuda.empty_cache()  # Clean cache in every step for memory saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYazbv-mQgqe",
        "outputId": "6413557b-8079-4ee4-b3e8-501e688ac7ea"
      },
      "outputs": [],
      "source": [
        "len(X_train_features_maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCLqbQsU9rjT",
        "outputId": "62391d9b-1ff6-43de-f22e-90e9fbf8697a"
      },
      "outputs": [],
      "source": [
        "X_train_features_maps[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRHI34IE9uMQ"
      },
      "outputs": [],
      "source": [
        "X_train_features = [list(dic.values()) for dic in X_train_features_maps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eWIgjAU-UW9",
        "outputId": "244db606-1025-425a-e02f-5ac6632c4fe5"
      },
      "outputs": [],
      "source": [
        "len(X_train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFXzlfwd-UYy",
        "outputId": "dd65859a-96e9-4b6a-8bdd-0d340eac1bce"
      },
      "outputs": [],
      "source": [
        "X_train_features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgd5gk9SjjK"
      },
      "source": [
        "## Training Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UdbK2IsvQgsp",
        "outputId": "bc48fbde-6eb7-49b7-c74b-3c1e6540fc11"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(verbose=1)\n",
        "clf.fit(X_train_features, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O69UE4MUS5_v"
      },
      "source": [
        "## Evaluate accuracy of Logistic Regression on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA6ovXb2S5kQ",
        "outputId": "a1172515-4b1e-4d36-f582-edf438caa21f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Y_Pred = clf.predict(X_train_features)\n",
        "\n",
        "accuracy = accuracy_score(Y_train, Y_Pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "asLAVNjYnMzX",
        "outputId": "976a42f0-f220-4bf7-b6ee-6865271abf8b"
      },
      "outputs": [],
      "source": [
        "log_odds = clf.coef_[0]\n",
        "odds = np.exp(clf.coef_[0])\n",
        "lr_features_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), log_odds)}\n",
        "lr_features_no_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), odds)}\n",
        "\n",
        "print(\"log\", lr_features_log)\n",
        "print(\"no_log\", lr_features_no_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCtQNHq9zVDv"
      },
      "source": [
        "## Extracting the Features of the Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-kRb2gzUE1",
        "outputId": "598aa3d8-b05b-456f-e749-afa3d5655973"
      },
      "outputs": [],
      "source": [
        "X_val_features_map = []\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_val, desc=\"Processing\"):\n",
        "    X_val_features_map.append(\n",
        "        extract_features(\"\", conditioned_text, generated_text, features_to_extract)\n",
        "    )\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI45BIF9zUHO"
      },
      "outputs": [],
      "source": [
        "X_val_features = [list(dic.values()) for dic in X_val_features_map]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uncomment nex cell if you have a validation set and you want to see LR accuracy on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "W4t1F1y9zUJf",
        "outputId": "c85b0afd-8704-48ba-d417-03cbef42d33a"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Y_Pred = clf.predict(X_val_features)\n",
        "\n",
        "# accuracy = accuracy_score(Y_val, Y_Pred)\n",
        "# print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITx_0w7BTCce"
      },
      "source": [
        "## Extracting the Features of the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg60b4WcS5Nw",
        "outputId": "68a385eb-cbf3-4d90-c2ab-6912397e5e09"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_test_features_map = []\n",
        "\n",
        "for conditioned_text, generated_text in tqdm(X_test, desc=\"Processing\"):\n",
        "    X_test_features_map.append(\n",
        "        extract_features(\n",
        "            \"\", conditioned_text, generated_text, features_to_extract\n",
        "        )\n",
        "    )\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOmEOFRY-fci"
      },
      "outputs": [],
      "source": [
        "X_test_features = [list(dic.values()) for dic in X_test_features_map]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaW4-c3XSzfP"
      },
      "source": [
        "## Evaluate accuracy of the LogisticRegression on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6_VRaKTQgu8",
        "outputId": "54bf8d4e-8a6b-4cee-cebc-d0664e6f2d14"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Y_Pred = clf.predict(X_test_features)\n",
        "\n",
        "lr_accuracy = accuracy_score(Y_test, Y_Pred)\n",
        "print(f\"Accuracy: {lr_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "8pMh03rcrs6m",
        "outputId": "cf86e151-5518-42a0-be4c-a33c2b23af9d"
      },
      "outputs": [],
      "source": [
        "log_odds = clf.coef_[0]\n",
        "pd.DataFrame(log_odds,\n",
        "             X_train_features_maps[0].keys(),\n",
        "             columns=['coef'])\\\n",
        "            .sort_values(by='coef', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "E4b53IT53_B7",
        "outputId": "4bec1773-3965-4284-d5a4-569912c378eb"
      },
      "outputs": [],
      "source": [
        "odds = np.exp(clf.coef_[0])\n",
        "pd.DataFrame(odds,\n",
        "             X_train_features_maps[0].keys(),\n",
        "             columns=['coef'])\\\n",
        "            .sort_values(by='coef', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FFCFqGLTMez"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleDenseNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1, dropout_prob=0.3):\n",
        "        super(SimpleDenseNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DICECZu0Qgw6"
      },
      "outputs": [],
      "source": [
        "denseModel = SimpleDenseNet(input_dim=len(list(features_to_extract.keys())), hidden_dim=512).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FeJQkPQQjFM"
      },
      "source": [
        "# Code declaring and computing all the metrics to measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb_0po_9QiKs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        "    auc\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(model, input_tensor, true_labels):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predicted_probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "        predicted = (outputs > 0.5).float().cpu().numpy()\n",
        "\n",
        "        true_labels = true_labels.cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(true_labels, predicted)\n",
        "        precision = precision_score(true_labels, predicted)\n",
        "        recall = recall_score(true_labels, predicted)\n",
        "        f1 = f1_score(true_labels, predicted)\n",
        "\n",
        "        precision_negative = precision_score(true_labels, predicted, pos_label=0)\n",
        "        recall_negative = recall_score(true_labels, predicted, pos_label=0)\n",
        "        f1_negative = f1_score(true_labels, predicted, pos_label=0)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(true_labels, predicted).ravel()\n",
        "        roc_auc = roc_auc_score(true_labels, predicted_probs)\n",
        "\n",
        "        P, R, _ = precision_recall_curve(true_labels, predicted, pos_label=1)\n",
        "        pr_auc = auc(R, P)\n",
        "\n",
        "        roc_auc_negative = roc_auc_score(\n",
        "            true_labels, 1 - predicted_probs\n",
        "        )  # If predicted_probs is the probability of the positive class\n",
        "        P_neg, R_neg, _ = precision_recall_curve(true_labels, predicted, pos_label=0)\n",
        "        pr_auc_negative = auc(R_neg, P_neg)\n",
        "\n",
        "        return {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1\": f1,\n",
        "            \"TP\": tp,\n",
        "            \"TN\": tn,\n",
        "            \"FP\": fp,\n",
        "            \"FN\": fn,\n",
        "            \"ROC AUC\": roc_auc,\n",
        "            \"PR AUC\": pr_auc,\n",
        "            \"Precision-Negative\": precision_negative,\n",
        "            \"Recall-Negative\": recall_negative,\n",
        "            \"F1-Negative\": f1_negative,\n",
        "            \"ROC AUC-Negative\": roc_auc_negative,\n",
        "            \"PR AUC-Negative\": pr_auc_negative,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Fi6-lJUP11"
      },
      "source": [
        "## Code for training the Dense Model and getting the result of all metrics corresponding to the Testing Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNtGdq4b-8wo",
        "outputId": "2ec2c5fc-14cc-482a-fd24-dbab9b9557f4"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, input_tensor, true_labels):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        correct = (predicted == true_labels).float().sum()\n",
        "        accuracy = correct / len(true_labels)\n",
        "        return accuracy.item()\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_features, dtype=torch.float32).to(device)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "print(X_train_tensor.shape, Y_train_tensor.shape)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(denseModel.parameters(), lr=0.001)\n",
        "\n",
        "bestValAcc = 0\n",
        "# Training loop\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    denseModel.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = denseModel(X_train_tensor)\n",
        "    loss = criterion(outputs, Y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute training accuracy\n",
        "    train_accuracy = compute_accuracy(denseModel, X_train_tensor, Y_train_tensor)\n",
        "\n",
        "    # Uncomment this if you want to see how the accuracy of testing improves during the training process.\n",
        "    ##Compute testing accuracy\n",
        "    # X_val_tensor = torch.tensor(X_val_features, dtype=torch.float32).to(device)\n",
        "    # Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    # val_accuracy = compute_accuracy(denseModel, X_val_tensor, Y_val_tensor)\n",
        "\n",
        "    # if bestValAcc < val_accuracy:\n",
        "    #     bestValAcc = val_accuracy\n",
        "    #     print(f'Saving model with best validation accuracy ...')\n",
        "    #     torch.save(denseModel.state_dict(), 'llama-' + task + '-best-model')\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.4f}\"\n",
        "        )\n",
        "        # \"Validation Accuracy\": {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE9X728RRHRK"
      },
      "source": [
        "# Compute the metrics using the model on the Test Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ntxC26m--ma",
        "outputId": "8c3a3435-7314-42cb-984b-f110120af1e4"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(X_test_features, dtype=torch.float32).to(device)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "test_metrics = compute_metrics(denseModel, X_test_tensor, Y_test_tensor)\n",
        "\n",
        "print(\n",
        "    f\"Testing - Accuracy: {test_metrics['Accuracy']:.4f}, Precision: {test_metrics['Precision']:.4f}, Recall: {test_metrics['Recall']:.4f}, F1: {test_metrics['F1']:.4f}, ROC AUC: {test_metrics['ROC AUC']:.4f}, PR AUC: {test_metrics['PR AUC']:.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"Testing - Negative: {test_metrics['Accuracy']:.4f}, Precision-Negative: {test_metrics['Precision-Negative']:.4f}, Recall-Negative: {test_metrics['Recall-Negative']:.4f}, F1-Negative: {test_metrics['F1-Negative']:.4f}, ROC AUC-Negative: {test_metrics['ROC AUC-Negative']:.4f}, PR AUC-Negative: {test_metrics['PR AUC-Negative']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-RlivuLRN0H"
      },
      "source": [
        "## Save the results on a CSV if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boqJczJoL4lW"
      },
      "outputs": [],
      "source": [
        "model_dataframe = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"features\",\n",
        "        \"model_name\",\n",
        "        \"feature_to_extract\",\n",
        "        \"method\",\n",
        "        \"accuracy\",\n",
        "        \"precision\",\n",
        "        \"recall\",\n",
        "        \"roc auc\",\n",
        "        \"pr auc\",\n",
        "        \"negative\",\n",
        "        \"precision-negative\",\n",
        "        \"recall-negative\",\n",
        "        \"negative f1\",\n",
        "        \"lr_accuracy\",\n",
        "        \"lr_features_log\",\n",
        "        \"lr_features_no_log\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJQ56NLzL5Tv"
      },
      "outputs": [],
      "source": [
        "d = {\n",
        "    \"features\": features_to_extract,\n",
        "    \"model_name\": str(model.getName()),\n",
        "    \"feature_to_extract\": feature_to_extract,\n",
        "    \"method\": \"TEST\",\n",
        "    \"accuracy\": test_metrics[\"Accuracy\"],\n",
        "    \"precision\": test_metrics[\"Precision\"],\n",
        "    \"recall\": test_metrics[\"Recall\"],\n",
        "    \"f1\": test_metrics[\"F1\"],\n",
        "    \"pr auc\": test_metrics[\"PR AUC\"],\n",
        "    \"precision-negative\": test_metrics[\"Precision-Negative\"],\n",
        "    \"recall-negative\": test_metrics[\"Recall-Negative\"],\n",
        "    \"negative-f1\": test_metrics[\"F1-Negative\"],\n",
        "    \"lr_accuracy\": lr_accuracy,\n",
        "    \"lr_features_log\": lr_features_log,\n",
        "    \"lr_features_no_log\": lr_features_no_log,\n",
        "}\n",
        "\n",
        "model_dataframe.loc[len(model_dataframe.index)] = d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNMNRvxvMO_o"
      },
      "outputs": [],
      "source": [
        "model_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tGhxo6SMRvc"
      },
      "outputs": [],
      "source": [
        "csv_name = f\"{model.getSanitizedName()}_mind_test={test_dataset[0]}_{includeConditioned=}_{'_'.join([f'{k}={v}' for k, v in features_to_extract.items()])}.csv\"\n",
        "model_dataframe.to_csv(output_path / csv_name, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
